{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c3b363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GaussianNoise, LSTM, Bidirectional, Dropout, Dense, Conv1D\n",
    "from tensorflow.keras import regularizers, backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, R2Score\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "\n",
    "# Append the project directory to sys.path\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "    \n",
    "from src.predictionModule.LoadupSamples import LoadupSamples\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb954e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"idxAfterPrediction\": 3,\n",
    "    'timesteps': 60,\n",
    "    'target_option': 'last',\n",
    "    \"LoadupSamples_time_scaling_stretch\": True,\n",
    "    \n",
    "    \"TreeTime_lstm_units\": 64,\n",
    "    \"TreeTime_lstm_num_layers\": 4,\n",
    "    \"TreeTime_lstm_dropout\": 0.00001,\n",
    "    \"TreeTime_lstm_recurrent_dropout\": 0.00001,\n",
    "    \"TreeTime_lstm_learning_rate\": 0.001,\n",
    "    \"TreeTime_lstm_optimizer\": \"adam\",\n",
    "    \"TreeTime_lstm_bidirectional\": True,\n",
    "    \"TreeTime_lstm_batch_size\": 2**12,\n",
    "    \"TreeTime_lstm_epochs\": 30,\n",
    "    \"TreeTime_lstm_l1\": 0.00001,\n",
    "    \"TreeTime_lstm_l2\": 0.00001,\n",
    "    \"TreeTime_inter_dropout\": 0.00001,\n",
    "    \"TreeTime_input_gaussian_noise\": 0.00001,\n",
    "    \"TreeTime_lstm_conv1d\": True,\n",
    "    \"TreeTime_lstm_conv1d_kernel_size\": 3,\n",
    "    \"TreeTime_lstm_loss\": \"mse\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c306cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_group = \"group_snp500_finanTo2011\"\n",
    "\n",
    "eval_date = datetime.date(year=2025, month=2, day=13)\n",
    "start_train_date = datetime.date(year=2014, month=1, day=1)\n",
    "\n",
    "\n",
    "params[\"idxAfterPrediction\"] = 5\n",
    "ls = LoadupSamples(\n",
    "    train_start_date=start_train_date,\n",
    "    test_dates=[eval_date],\n",
    "    group=stock_group,\n",
    "    params=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a74d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls.load_samples(main_path = \"../src/featureAlchemy/bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21232b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xtree = ls.train_Xtree\n",
    "train_Xtime = ls.train_Xtime\n",
    "\n",
    "treenames = ls.featureTreeNames\n",
    "timenames = ls.featureTimeNames\n",
    "\n",
    "train_ytree = ls.train_ytree\n",
    "train_ytime = ls.train_ytime\n",
    "\n",
    "meta = ls.meta_pl_train\n",
    "idx_after = params[\"idxAfterPrediction\"]\n",
    "train_yhot = meta.select([f\"target_close_at{i}\" for i in range(1, idx_after + 1)]).to_numpy()\n",
    "train_yhot = (train_yhot == train_yhot.max(axis=1, keepdims=True)).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55d8ede6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0025045\n",
      "0.5120161\n",
      "(668702, 5)\n"
     ]
    }
   ],
   "source": [
    "print(np.mean(train_ytree))\n",
    "print(np.mean(train_ytime))\n",
    "\n",
    "print(train_yhot.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "80dc05d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = np.where(timenames == \"MathFeature_TradedPrice\")[0][0]\n",
    "idx2 = np.where(timenames == \"FeatureTA_High\")[0][0]\n",
    "idx3 = np.where(timenames == \"FeatureTA_Low\")[0][0]\n",
    "idx4 = np.where(timenames == \"FeatureTA_volume_obv\")[0][0]\n",
    "idx5 = np.where(timenames == \"FeatureTA_volume_vpt\")[0][0]\n",
    "\n",
    "#train_Xtime = train_Xtime[:, :, [idx1, idx2, idx3, idx4, idx5]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "53f43e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm, trange\n",
    "import shap\n",
    "\n",
    "class TreeTimeLSTM(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 lstm_units,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 recurrent_dropout,\n",
    "                 bidirectional,\n",
    "                 output_size,         # new: number of classes m\n",
    "                 l1=0.0,\n",
    "                 l2=0.0,\n",
    "                 use_conv1d=False,\n",
    "                 conv_kernel=3,\n",
    "                 noise_std=0.0,\n",
    "                 inter_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.use_conv1d = use_conv1d\n",
    "        self.noise_std = noise_std\n",
    "        self.inter_dropout = inter_dropout\n",
    "\n",
    "        if use_conv1d:\n",
    "            self.conv1d = nn.Conv1d(\n",
    "                in_channels=input_size,\n",
    "                out_channels=lstm_units,\n",
    "                kernel_size=conv_kernel,\n",
    "                padding=conv_kernel//2\n",
    "            )\n",
    "            input_size = lstm_units\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=lstm_units,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(inter_dropout) if inter_dropout > 0 else None\n",
    "\n",
    "        self.output = nn.Linear(\n",
    "            lstm_units * (2 if bidirectional else 1),\n",
    "            output_size        # now outputs m logits\n",
    "        )\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.noise_std > 0:\n",
    "            x = x + torch.randn_like(x) * self.noise_std\n",
    "        if self.use_conv1d:\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.conv1d(x)\n",
    "            x = x.transpose(1, 2)\n",
    "        out, _ = self.lstm(x)\n",
    "        out_last = out[:, -1, :]\n",
    "        if self.dropout:\n",
    "            out_last = self.dropout(out_last)\n",
    "        return self.output(out_last)\n",
    "\n",
    "class TreeTimeGRU(nn.Module):\n",
    "    def __init__(self, \n",
    "                 input_size,\n",
    "                 lstm_units,\n",
    "                 num_layers,\n",
    "                 dropout,\n",
    "                 recurrent_dropout,\n",
    "                 bidirectional,\n",
    "                 output_size,         # new: number of classes m\n",
    "                 l1=0.0,\n",
    "                 l2=0.0,\n",
    "                 use_conv1d=False,\n",
    "                 conv_kernel=3,\n",
    "                 noise_std=0.0,\n",
    "                 inter_dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.use_conv1d = use_conv1d\n",
    "        self.noise_std = noise_std\n",
    "        self.inter_dropout = inter_dropout\n",
    "\n",
    "        if use_conv1d:\n",
    "            self.conv1d = nn.Conv1d(\n",
    "                in_channels=input_size,\n",
    "                out_channels=lstm_units,\n",
    "                kernel_size=conv_kernel,\n",
    "                padding=conv_kernel//2\n",
    "            )\n",
    "            input_size = lstm_units\n",
    "\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=input_size,\n",
    "            hidden_size=lstm_units,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout if num_layers > 1 else 0.0,\n",
    "            bidirectional=bidirectional,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(inter_dropout) if inter_dropout > 0 else None\n",
    "\n",
    "        self.output = nn.Linear(\n",
    "            lstm_units * (2 if bidirectional else 1),\n",
    "            output_size\n",
    "        )\n",
    "        self.l1 = l1\n",
    "        self.l2 = l2\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.noise_std > 0:\n",
    "            x = x + torch.randn_like(x) * self.noise_std\n",
    "        if self.use_conv1d:\n",
    "            x = x.transpose(1, 2)\n",
    "            x = self.conv1d(x)\n",
    "            x = x.transpose(1, 2)\n",
    "        out, _ = self.gru(x)\n",
    "        out_last = out[:, -1, :]\n",
    "        if self.dropout:\n",
    "            out_last = self.dropout(out_last)\n",
    "        return self.output(out_last)\n",
    "\n",
    "# Loss functions\n",
    "def quantile_loss(q):\n",
    "    def loss_fn(y_pred, y_true):\n",
    "        e = y_true - y_pred\n",
    "        return torch.mean(torch.max(q * e, (q - 1) * e))\n",
    "    return loss_fn\n",
    "\n",
    "def r2_metric(y_pred, y_true):\n",
    "    ss_res = torch.sum((y_true - y_pred) ** 2)\n",
    "    ss_tot = torch.sum((y_true - torch.mean(y_true)) ** 2)\n",
    "    return 1 - ss_res / (ss_tot + 1e-6)\n",
    "\n",
    "def neg_r2_loss(y_pred, y_true):\n",
    "    return -r2_metric(y_pred, y_true)\n",
    "\n",
    "\n",
    "def run(params, train_Xtime, train_ytime, training_ratio=0.90, device='cpu', type='lstm'):\n",
    "    num_classes = train_ytime.shape[1]\n",
    "    \n",
    "    # Hyperparameters\n",
    "    lstm_units = params['TreeTime_lstm_units']\n",
    "    num_layers = params['TreeTime_lstm_num_layers']\n",
    "    dropout = params['TreeTime_lstm_dropout']\n",
    "    recurrent_dropout = params['TreeTime_lstm_recurrent_dropout']\n",
    "    learning_rate = params['TreeTime_lstm_learning_rate']\n",
    "    optimizer_name = params['TreeTime_lstm_optimizer']\n",
    "    bidirectional = params['TreeTime_lstm_bidirectional']\n",
    "    batch_size = params['TreeTime_lstm_batch_size']\n",
    "    epochs = params['TreeTime_lstm_epochs']\n",
    "    l1 = params.get('TreeTime_lstm_l1', 0.0)\n",
    "    l2 = params.get('TreeTime_lstm_l2', 0.0)\n",
    "    inter_dropout = params.get('TreeTime_inter_dropout', 0.0)\n",
    "    noise_std = params.get('TreeTime_input_gaussian_noise', 0.0)\n",
    "    use_conv1d = params.get('TreeTime_lstm_conv1d', False)\n",
    "    conv_kernel = params.get('TreeTime_lstm_conv1d_kernel_size', 3)\n",
    "\n",
    "    # Data split\n",
    "    split_at = int(train_Xtime.shape[0] * training_ratio)\n",
    "    X_train, y_train = train_Xtime[:split_at], train_ytime[:split_at]\n",
    "    X_val,   y_val   = train_Xtime[split_at:],   train_ytime[split_at:]\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                      torch.tensor(y_train, dtype=torch.float32)),\n",
    "        batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
    "                      torch.tensor(y_val, dtype=torch.float32)),\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Model\n",
    "    if type == 'gru':\n",
    "        model = TreeTimeGRU(\n",
    "            input_size=X_train.shape[-1],\n",
    "            lstm_units=lstm_units,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            output_size=num_classes,    # <-- here\n",
    "            l1=l1,\n",
    "            l2=l2,\n",
    "            use_conv1d=use_conv1d,\n",
    "            conv_kernel=conv_kernel,\n",
    "            noise_std=noise_std,\n",
    "            inter_dropout=inter_dropout\n",
    "        ).to(device)\n",
    "    else:  # default to LSTM\n",
    "        model = TreeTimeLSTM(\n",
    "            input_size=X_train.shape[-1],\n",
    "            lstm_units=lstm_units,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            bidirectional=bidirectional,\n",
    "            output_size=num_classes,    # <-- here\n",
    "            l1=l1,\n",
    "            l2=l2,\n",
    "            use_conv1d=use_conv1d,\n",
    "            conv_kernel=conv_kernel,\n",
    "            noise_std=noise_std,\n",
    "            inter_dropout=inter_dropout\n",
    "        ).to(device)\n",
    "\n",
    "    # Loss: BCEWithLogits for one-hot targets\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Optimizer & scheduler as before…\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    if optimizer_name == 'rmsprop':\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=l2)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2)\n",
    "\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': []}\n",
    "    best_loss, wait = float('inf'), 0\n",
    "\n",
    "    best_loss, wait = float('inf'), 0\n",
    "    for epoch in range(1, epochs+1):\n",
    "        # --- Training ---\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for Xb, yb in train_loader:\n",
    "            Xb, yb = Xb.to(device), yb.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(Xb)\n",
    "            loss = criterion(logits, yb)\n",
    "            if l1>0:\n",
    "                l1_pen = sum(p.abs().sum() for p in model.parameters())\n",
    "                loss += l1 * l1_pen\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        avg_train = sum(train_losses) / len(train_losses)\n",
    "\n",
    "        # --- Validation ---\n",
    "        model.eval()\n",
    "        val_losses, correct, total = [], 0, 0\n",
    "        with torch.no_grad():\n",
    "            for Xb, yb in val_loader:\n",
    "                Xb, yb = Xb.to(device), yb.to(device)\n",
    "                logits = model(Xb)\n",
    "                loss = criterion(logits, yb)\n",
    "                val_losses.append(loss.item())\n",
    "                preds = logits.argmax(dim=1)\n",
    "                targets = yb.argmax(dim=1)\n",
    "                correct += (preds == targets).sum().item()\n",
    "                total += yb.size(0)\n",
    "        avg_val = sum(val_losses) / len(val_losses)\n",
    "        val_acc = correct / total\n",
    "\n",
    "        # record & print\n",
    "        history['train_loss'].append(avg_train)\n",
    "        history['val_loss'].append(avg_val)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        print(f\"Epoch {epoch}/{epochs} — \"\n",
    "              f\"train_loss: {avg_train:.4f}, \"\n",
    "              f\"val_loss: {avg_val:.4f}, \"\n",
    "              f\"val_acc: {val_acc:.4f}\")\n",
    "\n",
    "        scheduler.step(avg_val)\n",
    "        # early stopping\n",
    "        if avg_val < best_loss:\n",
    "            best_loss, best_state = avg_val, model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= 3: break\n",
    "\n",
    "    model.load_state_dict(best_state)\n",
    "    return best_loss, val_acc, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aec092cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 — train_loss: 1.6915, val_loss: 1.5757, val_acc: 0.2972\n",
      "Epoch 2/30 — train_loss: 1.6064, val_loss: 1.5754, val_acc: 0.2972\n",
      "Epoch 3/30 — train_loss: 1.5926, val_loss: 1.5752, val_acc: 0.2972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m device = \u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m best_loss, val_acc, model = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_Xtime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_yhot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_ratio\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlstm\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mValidation RMSE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 240\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(params, train_Xtime, train_ytime, training_ratio, device, type)\u001b[39m\n\u001b[32m    238\u001b[39m     loss.backward()\n\u001b[32m    239\u001b[39m     optimizer.step()\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m     train_losses.append(\u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    241\u001b[39m avg_train = \u001b[38;5;28msum\u001b[39m(train_losses) / \u001b[38;5;28mlen\u001b[39m(train_losses)\n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# --- Validation ---\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "best_loss, val_acc, model = run(params, train_Xtime, train_yhot, training_ratio=0.9, device=device, type='lstm')\n",
    "print(f\"Validation RMSE: {val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30e3be8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error: 0.0465\n",
      "Mean all prediction: 1.0001\n",
      "Mean above prediction: 1.0001\n",
      "Mean below prediction: 1.0001\n",
      "True values above zero: 1.0000\n",
      "True values below zero: 1.0001\n"
     ]
    }
   ],
   "source": [
    "X_new = np.random.randn(5, train_Xtime.shape[1], train_Xtime.shape[2])\n",
    "\n",
    "# Convert to torch tensor and send to device\n",
    "n = 20000\n",
    "X_tensor = torch.tensor(train_Xtime[-n:], dtype=torch.float32).to(device)\n",
    "\n",
    "# Put model into eval mode and disable grad\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    preds = model(X_tensor)        # (N, 1) tensor\n",
    "    preds = preds.squeeze(-1)      # (N,) tensor\n",
    "\n",
    "# Bring back to CPU NumPy array if you like\n",
    "preds = preds.cpu().numpy()\n",
    "\n",
    "true_val = train_ytree[-n:]\n",
    "\n",
    "rsme_err = np.sqrt(np.mean((preds - train_ytime[-n:])**2))\n",
    "q = 0.995\n",
    "mask_pred_above = preds >= np.quantile(preds, q)\n",
    "mask_pred_below = preds <= np.quantile(preds, 1-q)\n",
    "print(f\"Mean error: {rsme_err:.4f}\")\n",
    "print(f\"Mean all prediction: {np.mean(true_val):.4f}\")\n",
    "print(f\"Mean above prediction: {np.mean(true_val[mask_pred_above]):.4f}\")\n",
    "print(f\"Mean below prediction: {np.mean(true_val[mask_pred_below]):.4f}\")\n",
    "print(f\"True values above zero: {np.sum(mask_pred_above)/len(mask_pred_above):.4f}\")\n",
    "print(f\"True values below zero: {np.sum(true_val[mask_pred_below])/len(mask_pred_below):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9abd22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def explain_grad_input(model, test_data, device='cpu'):\n",
    "    \"\"\"\n",
    "    Compute feature importances via Gradient x Input for an LSTM model.\n",
    "\n",
    "    model: trained TreeTimeLSTM\n",
    "    test_data: numpy array (N, seq, features)\n",
    "    \"\"\"\n",
    "    model.to(device).eval()\n",
    "    inputs = torch.tensor(test_data, dtype=torch.float32, device=device, requires_grad=True)\n",
    "    \n",
    "    # forward pass\n",
    "    outputs = model(inputs).squeeze()               # (N,)\n",
    "    \n",
    "    # backpropagate to get ∂y/∂x\n",
    "    grads = torch.autograd.grad(\n",
    "        outputs,\n",
    "        inputs,\n",
    "        grad_outputs=torch.ones_like(outputs),\n",
    "        retain_graph=False,\n",
    "        create_graph=False\n",
    "    )[0]                                             # shape: (N, seq, features)\n",
    "    \n",
    "    # gradient × input\n",
    "    attributions = grads * inputs                   # elementwise\n",
    "    # aggregate over sequence dimension to get per-feature importance\n",
    "    feature_importance = attributions.abs().mean(dim=1)  # shape: (N, features)\n",
    "    \n",
    "    return feature_importance.cpu().detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9622148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importances shape: (999, 5)\n",
      "MathFeature_TradedPrice: 0.0000\n",
      "MathFeature_TradedPrice_sp0: 0.0000\n",
      "MathFeature_TradedPrice_sp1: 0.0000\n",
      "MathFeature_Return: 0.0000\n",
      "MathFeature_PriceAdjustment: 0.0000\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 5 is out of bounds for axis 0 with size 5",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m importances_mean = importances.mean(axis=\u001b[32m0\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(timenames):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mimportances_mean\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m*\u001b[32m1e5\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: index 5 is out of bounds for axis 0 with size 5"
     ]
    }
   ],
   "source": [
    "# usage\n",
    "test_samples = train_Xtime[-1000:-1]\n",
    "importances = explain_grad_input(model, test_samples)\n",
    "print(\"Importances shape:\", importances.shape)  # -> (5, n_features)\n",
    "\n",
    "importances_mean = importances.mean(axis=0)\n",
    "\n",
    "for i, name in enumerate(timenames):\n",
    "    print(f\"{name}: {importances_mean[i]*1e5:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48ce653f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE on sample: 0.0234\n",
      "Mean on masked : nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimer\\AppData\\Local\\Temp\\ipykernel_14096\\570207082.py:23: RuntimeWarning: Mean of empty slice.\n",
      "  mean_true_masked = (ytree_true[mask]).mean()\n",
      "c:\\Users\\kimer\\Desktop\\RandomOdyssey\\.venv\\Lib\\site-packages\\numpy\\_core\\_methods.py:147: RuntimeWarning: invalid value encountered in divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device).eval()\n",
    "\n",
    "# pick k random indices\n",
    "idx = random.sample(range(len(train_Xtime)), 200)\n",
    "Xtime_sample = train_Xtime[-1000:-1]\n",
    "ytime_true  = train_ytime[-1000:-1].reshape(-1)\n",
    "ytree_true = train_ytree[-1000:-1].reshape(-1)\n",
    "\n",
    "# prepare tensor\n",
    "X_tensor = torch.tensor(Xtime_sample, dtype=torch.float32)\n",
    "\n",
    "# tensor on correct device\n",
    "X_tensor = torch.from_numpy(Xtime_sample).float().to(device)\n",
    "with torch.no_grad():\n",
    "    preds = model(X_tensor).squeeze().cpu().numpy()\n",
    "    \n",
    "preds_tree = (preds-0.5)/5 +1.0\n",
    "rmse = np.sqrt(((ytree_true - preds_tree)**2).mean())\n",
    "\n",
    "print(f\"RMSE on sample: {rmse:.4f}\")\n",
    "\n",
    "mask = preds_tree > np.quantile(preds_tree, 0.8)\n",
    "mean_true_masked = (ytree_true[mask]).mean()\n",
    "print(f\"Mean on masked : {mean_true_masked:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
