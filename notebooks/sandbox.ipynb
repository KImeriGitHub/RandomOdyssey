{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "\n",
    "# Append the project directory to sys.path\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import datetime as dt\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "from dataclasses import asdict, is_dataclass, dataclass\n",
    "import os\n",
    "import json\n",
    "import pickle\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "import scipy\n",
    "import numpy as np\n",
    "from scipy.stats import linregress\n",
    "import bisect\n",
    "from scipy import stats\n",
    "\n",
    "from src.mathTools.SpecialMatrix import SpecialMatrix\n",
    "from src.mathTools.DistributionTools import DistributionTools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (4, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>foo</th><th>bar</th><th>ham</th><th>apple</th><th>bar_other</th></tr><tr><td>i64</td><td>f64</td><td>str</td><td>str</td><td>f64</td></tr></thead><tbody><tr><td>1</td><td>6.0</td><td>&quot;a&quot;</td><td>&quot;x&quot;</td><td>6.0</td></tr><tr><td>2</td><td>7.0</td><td>&quot;b&quot;</td><td>&quot;y&quot;</td><td>-2.0</td></tr><tr><td>null</td><td>null</td><td>&quot;d&quot;</td><td>&quot;z&quot;</td><td>-3.0</td></tr><tr><td>3</td><td>8.0</td><td>&quot;c&quot;</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (4, 5)\n",
       "┌──────┬──────┬─────┬───────┬───────────┐\n",
       "│ foo  ┆ bar  ┆ ham ┆ apple ┆ bar_other │\n",
       "│ ---  ┆ ---  ┆ --- ┆ ---   ┆ ---       │\n",
       "│ i64  ┆ f64  ┆ str ┆ str   ┆ f64       │\n",
       "╞══════╪══════╪═════╪═══════╪═══════════╡\n",
       "│ 1    ┆ 6.0  ┆ a   ┆ x     ┆ 6.0       │\n",
       "│ 2    ┆ 7.0  ┆ b   ┆ y     ┆ -2.0      │\n",
       "│ null ┆ null ┆ d   ┆ z     ┆ -3.0      │\n",
       "│ 3    ┆ 8.0  ┆ c   ┆ null  ┆ null      │\n",
       "└──────┴──────┴─────┴───────┴───────────┘"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"foo\": [1, 2, 3],\n",
    "        \"bar\": [6.0, 7.0, 8.0],\n",
    "        \"ham\": [\"a\", \"b\", \"c\"],\n",
    "    }\n",
    ")\n",
    "\n",
    "other_df = pl.DataFrame(\n",
    "    {\n",
    "        \"apple\": [\"x\", \"y\", \"z\"],\n",
    "        \"ham\": [\"a\", \"b\", \"d\"],\n",
    "        \"bar\": [6.0, -2.0, -3.0],\n",
    "    }\n",
    ")\n",
    "df.join(other_df, on=\"ham\", how=\"full\", coalesce=True, suffix=\"_other\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>a</th><th>b</th></tr><tr><td>i64</td><td>str</td></tr></thead><tbody><tr><td>30</td><td>&quot;z&quot;</td></tr><tr><td>10</td><td>&quot;x&quot;</td></tr><tr><td>50</td><td>&quot;v&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 2)\n",
       "┌─────┬─────┐\n",
       "│ a   ┆ b   │\n",
       "│ --- ┆ --- │\n",
       "│ i64 ┆ str │\n",
       "╞═════╪═════╡\n",
       "│ 30  ┆ z   │\n",
       "│ 10  ┆ x   │\n",
       "│ 50  ┆ v   │\n",
       "└─────┴─────┘"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pl.DataFrame({\n",
    "    \"a\": [10,20,30,40,50],\n",
    "    \"b\": [\"x\",\"y\",\"z\",\"w\",\"v\"]\n",
    "})\n",
    "\n",
    "indices = [2, 0, 4]             # arbitrary order\n",
    "filtered_df = df[indices]\n",
    "filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "  Library  Avg Time (s)\n",
      "0   NumPy      0.278182\n",
      "1  Polars      0.100574\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Quantile Benchmark: NumPy vs Polars\n",
    "# This cell measures the time to compute the 0.1 (10th percentile) quantile for each column\n",
    "# in a 2D NumPy array versus a Polars DataFrame.\n",
    "\n",
    "# %% [code]\n",
    "import os\n",
    "os.environ[\"POLARS_MAX_THREADS\"] = \"1\"\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import timeit\n",
    "print(pl.thread_pool_size())\n",
    "\n",
    "# Setup\n",
    "N = 100_000   # number of rows\n",
    "M = 100          # number of columns\n",
    "arr = np.random.rand(N, M)\n",
    "df = pl.DataFrame({f\"col{i}\": arr[:, i] for i in range(M)})\n",
    "\n",
    "reps = 10\n",
    "\n",
    "# NumPy quantile timing\n",
    "np_q_time = timeit.timeit(\n",
    "    'np.quantile(arr, 0.1, axis=0)',\n",
    "    globals=globals(),\n",
    "    number=reps\n",
    ") / reps\n",
    "\n",
    "# Polars quantile timing\n",
    "pl_q_time = timeit.timeit(\n",
    "    'df = pl.DataFrame({f\"col{i}\": arr[:, i] for i in range(M)}); df.quantile(0.1)',\n",
    "    globals=globals(),\n",
    "    number=reps\n",
    ") / reps\n",
    "\n",
    "# Display results\n",
    "results = pd.DataFrame({\n",
    "    'Library': ['NumPy', 'Polars'],\n",
    "    'Avg Time (s)': [np_q_time, pl_q_time]\n",
    "})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Library  Avg Time (s)\n",
      "0   NumPy      0.086293\n",
      "1  Polars      0.003414\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Concatenation Benchmark: NumPy vs Polars\n",
    "\n",
    "# %% [code]\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "# Setup data\n",
    "N = 200_000\n",
    "arr_list = [np.random.rand(N) for _ in range(200)]\n",
    "\n",
    "reps = 50\n",
    "\n",
    "# NumPy concatenation\n",
    "np_cat_time = timeit.timeit('np.concatenate(arr_list)', globals=globals(), number=reps) / reps\n",
    "\n",
    "# Polars concatenation\n",
    "pl_cat_time = timeit.timeit('df_list = [pl.DataFrame({\"col\": arr}) for arr in arr_list]; pl.concat(df_list)', globals=globals(), number=reps) / reps\n",
    "\n",
    "# Show results\n",
    "results = pd.DataFrame({\n",
    "    'Library': ['NumPy', 'Polars'],\n",
    "    'Avg Time (s)': [np_cat_time, pl_cat_time]\n",
    "})\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Library  Avg Time (s)\n",
      "0   NumPy      0.000502\n",
      "1  Polars      0.000489\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import timeit\n",
    "\n",
    "# Setup data\n",
    "N = 100_000\n",
    "arr = np.random.rand(N)\n",
    "val = 0.5\n",
    "# Benchmark repetitions\n",
    "reps = 20\n",
    "\n",
    "# NumPy timing\n",
    "np_time = timeit.timeit('arr[arr > val]', globals=globals(), number=reps) / reps\n",
    "\n",
    "# Polars DataFrame filter timing\n",
    "pl_time = timeit.timeit('df = pl.DataFrame({\"col\": arr}); df.filter(pl.col(\"col\") > val); arr2 = df.to_numpy()', globals=globals(), number=reps) / reps\n",
    "\n",
    "# Prepare results\n",
    "results = pd.DataFrame({\n",
    "    'Library': ['NumPy', 'Polars'],\n",
    "    'Avg Time (s)': [np_time, pl_time]\n",
    "})\n",
    "\n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (2, 4)\n",
      "┌────────┬────────────┬───────┬──────────────┐\n",
      "│ ticker ┆ date       ┆ Close ┆ target_close │\n",
      "│ ---    ┆ ---        ┆ ---   ┆ ---          │\n",
      "│ str    ┆ date       ┆ f64   ┆ f64          │\n",
      "╞════════╪════════════╪═══════╪══════════════╡\n",
      "│ A      ┆ 2025-05-01 ┆ 10.0  ┆ 12.0         │\n",
      "│ A      ┆ 2025-05-03 ┆ 12.0  ┆ 12.0         │\n",
      "└────────┴────────────┴───────┴──────────────┘\n",
      "shape: (1, 4)\n",
      "┌────────┬────────────┬───────┬──────────────┐\n",
      "│ ticker ┆ date       ┆ Close ┆ target_close │\n",
      "│ ---    ┆ ---        ┆ ---   ┆ ---          │\n",
      "│ str    ┆ date       ┆ f64   ┆ f64          │\n",
      "╞════════╪════════════╪═══════╪══════════════╡\n",
      "│ B      ┆ 2025-05-01 ┆ 5.0   ┆ 5.0          │\n",
      "└────────┴────────────┴───────┴──────────────┘\n",
      "shape: (4, 4)\n",
      "┌────────┬────────────┬───────┬──────────────┐\n",
      "│ ticker ┆ date       ┆ Close ┆ target_close │\n",
      "│ ---    ┆ ---        ┆ ---   ┆ ---          │\n",
      "│ str    ┆ date       ┆ i64   ┆ i64          │\n",
      "╞════════╪════════════╪═══════╪══════════════╡\n",
      "│ A      ┆ 2025-05-01 ┆ 1     ┆ 2            │\n",
      "│ A      ┆ 2025-05-02 ┆ 2     ┆ 2            │\n",
      "│ B      ┆ 2025-05-01 ┆ 3     ┆ 3            │\n",
      "│ B      ┆ 2025-05-04 ┆ 4     ┆ 4            │\n",
      "└────────┴────────────┴───────┴──────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kimer\\AppData\\Local\\Temp\\ipykernel_20056\\2338872466.py:18: UserWarning: Sortedness of columns cannot be checked when 'by' groups provided\n",
      "  .join_asof(\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import polars as pl\n",
    "from datetime import date\n",
    "\n",
    "def __add_target(meta_pl_all: pl.DataFrame, days_After: int = 2):\n",
    "    meta_pl_all = meta_pl_all.with_row_index(\"row_index\")\n",
    "    meta_pl_all = meta_pl_all.with_columns(\n",
    "        (pl.col(\"date\") + pl.duration(days=days_After)).alias(\"target_date\")\n",
    "    )\n",
    "    meta_pl_all = meta_pl_all.sort([\"ticker\", \"date\"])\n",
    "    prices = (\n",
    "        meta_pl_all\n",
    "        .select([\"ticker\",\"date\",\"Close\"])\n",
    "        .rename({\"date\":\"price_date\",\"Close\":\"price\"})\n",
    "    )\n",
    "    return (\n",
    "        meta_pl_all\n",
    "        .join_asof(\n",
    "            prices,\n",
    "            left_on=\"target_date\",\n",
    "            right_on=\"price_date\",\n",
    "            by=\"ticker\",\n",
    "            strategy=\"backward\",\n",
    "        )\n",
    "        .with_columns(pl.col(\"price\").alias(\"target_close\"))\n",
    "        .drop([\"price_date\",\"price\",\"target_date\",\"row_index\"])\n",
    "    )\n",
    "\n",
    "# %%\n",
    "# 1) Simple case: matching next-day price\n",
    "df1 = pl.DataFrame({\n",
    "    \"ticker\": [\"A\",\"A\"],\n",
    "    \"date\":    [date(2025,5,1), date(2025,5,3)],\n",
    "    \"Close\":   [10.0, 12.0]\n",
    "})\n",
    "print(__add_target(df1, days_After=2))\n",
    "\n",
    "# %%\n",
    "# 2) No future price available (edge: gap too large)\n",
    "df2 = pl.DataFrame({\n",
    "    \"ticker\": [\"B\"],\n",
    "    \"date\":    [date(2025,5,1)],\n",
    "    \"Close\":   [5.0]\n",
    "})\n",
    "print(__add_target(df2, days_After=3))\n",
    "# expect target_close = null\n",
    "\n",
    "# %%\n",
    "# 3) Multiple tickers and duplicate dates\n",
    "df3 = pl.DataFrame({\n",
    "    \"ticker\": [\"A\",\"A\",\"B\",\"B\"],\n",
    "    \"date\":    [date(2025,5,1), date(2025,5,2), date(2025,5,1), date(2025,5,4)],\n",
    "    \"Close\":   [1,2,3,4]\n",
    "})\n",
    "print(__add_target(df3, days_After=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../src/featureAlchemy/bin/TimeFeatures_meta_2015_group_snp500_finanTo2011.npz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 61\u001b[39m\n\u001b[32m     53\u001b[39m time_npz = \u001b[33m\"\u001b[39m\u001b[33m../src/featureAlchemy/bin/TimeFeatures_meta_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m.npz\u001b[39m\u001b[33m\"\u001b[39m.format(label, group)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# run checks\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#check_features(tree_npz,\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m#               meta_key=\"meta_tree\",\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m#               feat_key=\"treeFeatures\",\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m#               names_key=\"treeFeaturesNames\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m \u001b[43mcheck_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtime_npz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m               \u001b[49m\u001b[43mmeta_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmeta_time\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m               \u001b[49m\u001b[43mfeat_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimeFeatures\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m               \u001b[49m\u001b[43mnames_key\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtimeFeaturesNames\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     65\u001b[39m \u001b[43m               \u001b[49m\u001b[43mvalue_range\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mAll feature files look good!\u001b[39m\u001b[33m\"\u001b[39m)  \n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mcheck_features\u001b[39m\u001b[34m(npz_path, meta_key, feat_key, names_key, value_range)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_features\u001b[39m(npz_path, meta_key, feat_key, names_key, value_range=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     data = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnpz_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m     meta   = data[meta_key]\n\u001b[32m      6\u001b[39m     feats  = data[feat_key]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\kimer\\Desktop\\RandomOdyssey\\.venv\\Lib\\site-packages\\numpy\\lib\\_npyio_impl.py:459\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    457\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m459\u001b[39m     fid = stack.enter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m    460\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    462\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '../src/featureAlchemy/bin/TimeFeatures_meta_2015_group_snp500_finanTo2011.npz'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def check_features(npz_path, meta_key, feat_key, names_key, value_range=None):\n",
    "    data = np.load(npz_path,allow_pickle=True )\n",
    "    meta   = data[meta_key]\n",
    "    feats  = data[feat_key]\n",
    "    names  = data[names_key]\n",
    "\n",
    "    # non-empty\n",
    "    assert meta.size > 0,      f\"{meta_key} is empty\"\n",
    "    assert feats.size > 0,     f\"{feat_key} is empty\"\n",
    "    assert names.size > 0,     f\"{names_key} is empty\"\n",
    "\n",
    "    # column counts\n",
    "    assert meta.shape[0] == feats.shape[0], (\n",
    "        f\"{meta_key}.shape[0]={meta.shape[0]} ≠ {feat_key}.shape[0]={feats.shape[0]}\"\n",
    "    )\n",
    "    \n",
    "    def assert_no_empty(arr: np.array, name):\n",
    "        # numeric arrays: check NaN\n",
    "        if np.issubdtype(arr.dtype, np.number):\n",
    "            assert np.all(np.isfinite(arr)), f\"{name} contains NaN or infinities\"\n",
    "        # object/string arrays: check empty or None\n",
    "        else:\n",
    "            bad = [i for i, v in enumerate(arr.ravel()) if v is None or v == \"\"]\n",
    "            assert not bad, f\"{name} has empty entries at flat indices {bad}\"\n",
    "\n",
    "    # tree block\n",
    "    if \"time\" in feat_key:\n",
    "        assert_no_empty(data[feat_key], feat_key)\n",
    "    \n",
    "    if \"tree\" in feat_key:\n",
    "        assert feats.shape[1] == len(names), (\n",
    "            f\"{feat_key}.shape[1]={feats.shape[1]} ≠ len({names_key})={len(names)}\"\n",
    "        )\n",
    "\n",
    "    if \"time\" in feat_key:\n",
    "        assert feats.shape[2] == len(names), (\n",
    "            f\"{feat_key}.shape[2]={feats.shape[2]} ≠ len({names_key})={len(names)}\"\n",
    "        )\n",
    "\n",
    "    # optional value-range check\n",
    "    if value_range:\n",
    "        lo, hi = value_range\n",
    "        assert np.all(feats >= lo) and np.all(feats <= hi), (\n",
    "            f\"{feat_key} values not in [{lo}, {hi}]\"\n",
    "        )\n",
    "\n",
    "# paths\n",
    "label = 2015\n",
    "group = \"group_snp500_finanTo2011\"\n",
    "tree_npz = \"../src/featureAlchemy/bin/TreeFeatures_{}_{}.npz\".format(label, group)\n",
    "time_npz = \"../src/featureAlchemy/bin/TimeFeatures_meta_{}_{}.npz\".format(label, group)\n",
    "\n",
    "# run checks\n",
    "#check_features(tree_npz,\n",
    "#               meta_key=\"meta_tree\",\n",
    "#               feat_key=\"treeFeatures\",\n",
    "#               names_key=\"treeFeaturesNames\")\n",
    "\n",
    "check_features(time_npz,\n",
    "               meta_key=\"meta_time\",\n",
    "               feat_key=\"timeFeatures\",\n",
    "               names_key=\"timeFeaturesNames\",\n",
    "               value_range=(0.0, 1.0))\n",
    "\n",
    "print(\"All feature files look good!\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 feature(s) out of [0.0, 1.0]:\n",
      " • #9 Fourier_Price_RSMERatioCoeff_1_MH_1: min=-0.383, max=0.462\n",
      " • #10 Fourier_Price_RSMERatioCoeff_2_MH_1: min=-0.326, max=0.462\n",
      " • #11 Fourier_Price_RSMERatioCoeff_3_MH_1: min=-0.247, max=0.463\n",
      " • #12 Fourier_Price_RSMERatioCoeff_4_MH_1: min=-0.246, max=0.463\n"
     ]
    }
   ],
   "source": [
    "time_npz = f\"../src/featureAlchemy/bin/TimeFeatures_meta_{label}_{group}.npz\"\n",
    "data     = np.load(time_npz, allow_pickle=True)\n",
    "met = data['meta_time']\n",
    "feats    = data['timeFeatures']\n",
    "names    = data['timeFeaturesNames']\n",
    "\n",
    "# ─── Value-range check ─────────────────────────────────────────────────────────\n",
    "lo, hi = 0.0, 1.0\n",
    "\n",
    "# reshape so each column is a feature\n",
    "flat = feats.reshape(-1, feats.shape[-1])\n",
    "\n",
    "# compute per-column min/max\n",
    "mins = flat.min(axis=0)\n",
    "maxs = flat.max(axis=0)\n",
    "\n",
    "# find offending features\n",
    "bad = [\n",
    "    (i, names[i], float(mins[i]), float(maxs[i]))\n",
    "    for i in range(len(names))\n",
    "    if mins[i] < lo or maxs[i] > hi\n",
    "]\n",
    "\n",
    "if bad:\n",
    "    print(f\"{len(bad)} feature(s) out of [{lo}, {hi}]:\")\n",
    "    for idx, name, mn, mx in bad:\n",
    "        print(f\" • #{idx} {name}: min={mn:.3g}, max={mx:.3g}\")\n",
    "else:\n",
    "    print(\"All timeFeatures ∈ [0,1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full dense matrix A_full:\n",
      " [[0.2        0.16666667 0.14285714 0.125      0.11111111 0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.        ]]\n",
      "All are close: True\n"
     ]
    }
   ],
   "source": [
    "num_sam_tr = 60  # matrix size\n",
    "n_diags = 4      # number of off-diagonals on each side\n",
    "A_sparse = SpecialMatrix.boundedDiagonal_oneNetCol(num_sam_tr, n_diags)\n",
    "\n",
    "# For demonstration: print the sparse matrix in dense form.\n",
    "A_full = A_sparse.todense()\n",
    "\n",
    "# --- 3. Print the matrices ---\n",
    "print(\"\\nFull dense matrix A_full:\\n\", A_full[0])\n",
    "#print(\"\\nSparse matrix A_sparse:\")\n",
    "#print(A_sparse)\n",
    "\n",
    "# --- 4. Matrix-vector multiplication ---\n",
    "x = np.random.rand(num_sam_tr)\n",
    "y_full = A_full @ x\n",
    "y_sparse = A_sparse @ x\n",
    "\n",
    "print(\"All are close:\", np.allclose(y_full, y_sparse))  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse matrix-vector multiplication (solving A*y = x):\n",
      "Absolute difference:\n",
      " 2600040400926568.0\n",
      "Dense matrix:\n",
      "  Numerical rank: 7\n",
      "  Determinant: 2.0706540538643876e-17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "matrix([[ 0.45067453],\n",
       "        [-0.14372927],\n",
       "        [-0.20424094],\n",
       "        [-0.54761511],\n",
       "        [ 0.49338753],\n",
       "        [ 0.45388748],\n",
       "        [-0.31895834],\n",
       "        [-0.18340588]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy.sparse.linalg as spla\n",
    "\n",
    "\n",
    "num_sam_tr = 8  # matrix size\n",
    "n_diags = 3      # number of off-diagonals on each side\n",
    "A_sparse = SpecialMatrix.boundedDiagonal_sparse(num_sam_tr, n_diags)\n",
    "\n",
    "# For demonstration: print the sparse matrix in dense form.\n",
    "A_full = A_sparse.todense()\n",
    "\n",
    "## --- 5. Inverse matrix-vector multiplication ---\n",
    "x = np.random.rand(num_sam_tr)\n",
    "## Solve A*y = x for y.\n",
    "y_inv_full = np.linalg.solve(A_full, x)\n",
    "y_inv_sparse = spla.spsolve(A_sparse, x)\n",
    "print(\"\\nInverse matrix-vector multiplication (solving A*y = x):\")\n",
    "print(\"Absolute difference:\\n\", np.mean(np.abs(y_inv_full - y_inv_sparse)))\n",
    "\n",
    "# --- Compute numerical rank and determinant for the dense matrix A_full ---\n",
    "U, s, Vh = np.linalg.svd(A_full)\n",
    "tol = max(A_full.shape) * np.amax(s) * np.finfo(s.dtype).eps\n",
    "num_rank_full = np.sum(s > tol)\n",
    "det_full = np.linalg.det(A_full)\n",
    "print(\"Dense matrix:\")\n",
    "print(\"  Numerical rank:\", num_rank_full)\n",
    "print(\"  Determinant:\", det_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result of circulant multiply: [ 0.21012606 -0.13314346  0.15737162  0.02087521 -0.03788862 -0.15362854\n",
      "  0.15314803 -0.16258518  0.37174242 -0.10257611 -0.03736177 -0.26782281\n",
      " -0.06814705  0.04932619  0.29093784 -0.02471498 -0.33416127  0.04366359\n",
      "  0.22688358 -0.20203418]\n",
      "Lowest eigenvalue: 9.999999999177334e-07\n",
      "Original x: [0.24748827 0.60572137 0.29671893 0.45556372 0.54210204 0.67540687\n",
      " 0.34681953 0.68082847 0.12293884 0.66881797 0.64903305 0.91469166\n",
      " 0.68131546 0.51208106 0.23120074 0.58238626 0.92121761 0.4864869\n",
      " 0.25784285 0.69901605]\n",
      "Recovered x: [0.24748827 0.60572137 0.29671893 0.45556372 0.54210204 0.67540687\n",
      " 0.34681953 0.68082847 0.12293884 0.66881797 0.64903305 0.91469166\n",
      " 0.68131546 0.51208106 0.23120074 0.58238626 0.92121761 0.4864869\n",
      " 0.25784285 0.69901605]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def circulant_multiply(c, x):\n",
    "    \"\"\"Multiply circulant matrix (defined by first column c) by vector x.\"\"\"\n",
    "    return np.real(np.fft.ifft(np.fft.fft(c) * np.fft.fft(x)))\n",
    "\n",
    "def circulant_inverse_multiply(c, b):\n",
    "    \"\"\"Solve Ax = b for circulant matrix A defined by first column c.\"\"\"\n",
    "    eigenvalues = np.fft.fft(c)\n",
    "    print(f\"Lowest eigenvalue: {np.min(np.abs(eigenvalues))}\")\n",
    "    if np.any(np.abs(eigenvalues) < 1e-12):\n",
    "        raise ValueError(\"Circulant matrix is singular!\")\n",
    "    return np.real(np.fft.ifft(np.fft.fft(b) / eigenvalues))\n",
    "\n",
    "def circulant_coeffs(n, a):\n",
    "    # Compute cyclic distances: d[k] = min(k, n-k)\n",
    "    d = np.minimum(np.arange(n), n - np.arange(n))\n",
    "    # Exponential decay: main diagonal is 1, neighbors decay as exp(-a*d)\n",
    "    coeffs = np.exp(-a * d)\n",
    "    # Normalize so the coefficients sum to 1\n",
    "    return coeffs / coeffs.sum()\n",
    "# Example usage:\n",
    "num_sam_tr = 20\n",
    "c = circulant_coeffs(num_sam_tr, 0.5)  # first column of the circulant matrix\n",
    "eps = 1e-6 # to avoid singular matrix\n",
    "c[0] -= 1 - eps\n",
    "x = np.random.rand(num_sam_tr)  # vector to multiply\n",
    "\n",
    "# Multiply circulant matrix by x\n",
    "y = circulant_multiply(c, x)\n",
    "print(\"Result of circulant multiply:\", y)\n",
    "\n",
    "# Now, solve Ax = y to recover x\n",
    "x_recovered = circulant_inverse_multiply(c, y)\n",
    "print(\"Original x:\", x)\n",
    "print(\"Recovered x:\", x_recovered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04678486138395458\n",
      "0.018274742528420162\n"
     ]
    }
   ],
   "source": [
    "num_sam_tr = 50000\n",
    "num_sam_te = 1000\n",
    "diagonal_num = 0\n",
    "n_feat = 10\n",
    "sam_tr = np.zeros((num_sam_tr, n_feat), dtype=float)\n",
    "sam_te = np.zeros((num_sam_te, n_feat), dtype=float)\n",
    "\n",
    "for i in range(n_feat):\n",
    "    sam_tr[:,i] = np.random.normal(loc=0-i/5000, scale=2+i/3000, size=num_sam_tr) #train\n",
    "    sam_te[:,i] = np.random.normal(loc=0+i/5000, scale=1-i/3000, size=num_sam_te) #test\n",
    "sam_te[:,-3] = np.random.normal(loc=0+0.3, scale=1, size=num_sam_te) + np.random.normal(loc=-0.3, scale=1.2, size=num_sam_te)\n",
    "sam_te[:,-2] = np.random.uniform(low=-0.5, high=0.5, size=num_sam_te)\n",
    "sam_te[:,-1] = np.random.uniform(low=-1.9, high=1.9, size=num_sam_te)\n",
    "\n",
    "argsort_sam_tr = np.argsort(sam_tr, axis=0)\n",
    "argsort_sam_te = np.argsort(sam_te, axis=0)\n",
    "re_argsort_sam_tr = np.argsort(argsort_sam_tr, axis=0)\n",
    "re_argsort_sam_te = np.argsort(argsort_sam_te, axis=0)\n",
    "sort_sam_tr = np.sort(sam_tr, axis=0)\n",
    "sort_sam_te = np.sort(sam_te, axis=0)\n",
    "\n",
    "ksDist = DistributionTools.ksDistance(sam_tr, sam_te, weights=None)\n",
    "print(ksDist.mean())\n",
    "\n",
    "importance = np.zeros(n_feat)\n",
    "importance[0] = 1\n",
    "s = DistributionTools.establishMatchingWeight(sam_tr, sam_te, 10, 0.4)\n",
    "#s = weights @ importance\n",
    "\n",
    "ksDist_s = DistributionTools.ksDistance(sam_tr, sam_te, weights=s)\n",
    "print(ksDist_s.mean())\n",
    "\n",
    "fig, axs = plt.subplots(n_feat, 1, figsize=(6, 4 * n_feat))\n",
    "for cube_idx in range(n_feat):\n",
    "    # sort s according to the training samples for this cube_idx\n",
    "    s_sorted = s[argsort_sam_tr[:, cube_idx]]\n",
    "    s_cum = np.cumsum(s_sorted) / np.sum(s_sorted)\n",
    "    \n",
    "    axs[cube_idx].scatter(sort_sam_tr[:, cube_idx], s_cum,\n",
    "                           label='S Distribution', marker='.', edgecolors='blue')\n",
    "    axs[cube_idx].scatter(sort_sam_te[:, cube_idx],\n",
    "                           np.linspace(0, 1, num_sam_te),\n",
    "                           label='Test Distribution', marker='.', edgecolors='red')\n",
    "    axs[cube_idx].set_title(f'Sample Distribution (cube_idx = {cube_idx})')\n",
    "    axs[cube_idx].set_xlabel('x')\n",
    "    axs[cube_idx].set_ylabel('Density')\n",
    "    axs[cube_idx].legend()\n",
    "\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.32109053, 0.34814003, 0.325643  , 0.09236356, 0.70709246,\n",
       "       0.3234183 ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ksDist_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 3]\n"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "\n",
    "def count_entries_between(v, w):\n",
    "    lo = np.searchsorted(w, v[:-1], side=\"right\")\n",
    "    hi = np.searchsorted(w, v[1:], side=\"left\")\n",
    "    return hi - lo\n",
    "\n",
    "# Example usage:\n",
    "v = [1, 3, 5, 10]\n",
    "w = [2, 4, 6, 7, 9]\n",
    "print(count_entries_between(v, w))\n",
    "\n",
    "w=np.array(w).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(0)\n",
    "num_sam_tr = 1000\n",
    "# Generate samples from a normal distribution\n",
    "data = np.random.normal(loc=0, scale=1, size=num_sam_tr)\n",
    "\n",
    "# Create random weights and normalize them to sum to 1\n",
    "weights = np.random.rand(num_sam_tr)+0.5\n",
    "weights /= weights.sum()\n",
    "\n",
    "# Sort data and associated weights\n",
    "sorted_indices = np.argsort(data)\n",
    "data_sorted = data[sorted_indices]\n",
    "weights_sorted = weights[sorted_indices]\n",
    "\n",
    "# Compute the weighted cumulative sum (CDF)\n",
    "cdf = np.cumsum(weights_sorted)\n",
    "\n",
    "# Plot the weighted CDF\n",
    "plt.step(data_sorted, cdf, where='post')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Weighted CDF')\n",
    "plt.title('Weighted CDF for Normal Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000.0\n",
      "100000.0\n",
      "100000.0\n"
     ]
    }
   ],
   "source": [
    "num_sam2 = 100000\n",
    "num_sam1 = 100000//10\n",
    "sam1 = np.random.normal(loc=0, scale=1, size=num_sam1) #test\n",
    "sam2 = np.random.normal(loc=0, scale=2, size=num_sam2) #train\n",
    "\n",
    "argsort_sam1 = np.argsort(sam1)\n",
    "argsort_sam2 = np.argsort(sam2)\n",
    "sort_sam1 = sam1[argsort_sam1]\n",
    "sort_sam2 = sam2[argsort_sam2]\n",
    "\n",
    "#diff_sort_sam1 = np.diff(sort_sam1)\n",
    "#diff_sort_sam2 = np.diff(sort_sam2)\n",
    "#diff_xpts = np.diff(xpts)\n",
    "#\n",
    "#quot_sam1 = diff_xpts / diff_sort_sam1\n",
    "#quot_sam2 = diff_xpts / diff_sort_sam2\n",
    "#\n",
    "#quot = quot_sam1 / quot_sam2\n",
    "\n",
    "def count_entries_between(v, w):\n",
    "    lo = np.searchsorted(w, v[:-1], side=\"right\")\n",
    "    hi = np.searchsorted(w, v[1:], side=\"left\")\n",
    "    return hi - lo\n",
    "def smooth_array(data, window_size=3):\n",
    "    # Create a uniform window\n",
    "    window = np.ones(window_size) / window_size\n",
    "    # Convolve while keeping the output length the same\n",
    "    return np.convolve(data, window, mode='same')\n",
    "\n",
    "weights2 = count_entries_between(sort_sam2, sort_sam1)\n",
    "weights2 = smooth_array(weights2, window_size = 10)\n",
    "#weights2 = np.ones(num_sam2)\n",
    "weights2 = np.insert(weights2, 0, weights2[0])\n",
    "weights2 *= num_sam2/np.sum(weights2)\n",
    "\n",
    "weights0 = np.ones(num_sam2)\n",
    "cdf2 = np.cumsum(weights2)\n",
    "cdf2 = cdf2 / cdf2[-1]\n",
    "weights2_org = weights2[np.argsort(argsort_sam1)]\n",
    "plt.plot(weights2)\n",
    "plt.show()\n",
    "\n",
    "print(np.cumsum(weights2)[-1])\n",
    "print(np.cumsum(weights0)[-1])\n",
    "print(np.sum(weights0))\n",
    "# Plot the weighted CDF\n",
    "plt.step(sort_sam2, cdf2, where='post', color='green')\n",
    "plt.xlabel('Data')\n",
    "plt.ylabel('Weighted CDF')\n",
    "plt.title('Weighted Cumulative Distribution Function')\n",
    "\n",
    "#isam1 = np.searchsorted(sort_sam1, xpts)\n",
    "\n",
    "plt.scatter(sort_sam1, np.linspace(0, 1, num_sam1), label='Sam test', marker='.', linestyle='--', linewidths=0.1, edgecolors='blue')\n",
    "plt.scatter(sort_sam2, np.linspace(0, 1, num_sam2), label='Sam train', marker='.', linestyle='--', linewidths=0.1, edgecolors='red')\n",
    "plt.title('samples')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('Density')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
