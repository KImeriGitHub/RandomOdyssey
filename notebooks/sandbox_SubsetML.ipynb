{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "\n",
    "# Append the project directory to sys.path\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kimer\\Desktop\\RandomOdyssey\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import asdict, is_dataclass, dataclass\n",
    "from pandas.api.types import is_datetime64_any_dtype\n",
    "import scipy\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "from typing import Dict, List\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, log_loss, confusion_matrix\n",
    "import bisect\n",
    "\n",
    "from src.common.AssetData import AssetData\n",
    "from src.common.AssetDataPolars import AssetDataPolars\n",
    "from src.common.AssetDataService import AssetDataService\n",
    "from src.common.AssetFileInOut import AssetFileInOut \n",
    "from src.databaseService.OutsourceLoader import OutsourceLoader\n",
    "\n",
    "from src.simulation.SimulatePortfolio import SimulatePortfolio\n",
    "from src.strategy.StratBuyAndHold import StratBuyAndHold\n",
    "from src.simulation.ResultAnalyzer import ResultAnalyzer\n",
    "from src.common.AssetFileInOut import AssetFileInOut\n",
    "from src.predictionModule.SubsetML import SubsetML\n",
    "from src.common.DataFrameTimeOperations import DataFrameTimeOperationsPandas as DFTO\n",
    "from src.predictionModule.CollectionModels import CollectionModels\n",
    "from src.common.AssetFileInOut import AssetFileInOut\n",
    "\n",
    "from src.common.AssetDataPolars import AssetDataPolars\n",
    "from src.common.AssetDataService import AssetDataService\n",
    "from src.predictionModule.ModelAnalyzer import ModelAnalyzer\n",
    "\n",
    "from src.featureAlchemy.FeatureMain import FeatureMain\n",
    "from src.common.DataFrameTimeOperations import DataFrameTimeOperationsPolars as DPl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assets=AssetFileInOut(\"../src/stockGroups/bin\").loadDictFromFile(\"group_finanTo2011\")\n",
    "\n",
    "# Convert to Polars for speedup\n",
    "assetspl: Dict[str, AssetDataPolars] = {}\n",
    "for ticker, asset in assets.items():\n",
    "    assetspl[ticker]= AssetDataService.to_polars(asset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsetML = SubsetML(assetspl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data and metadata loaded from ../src/predictionModule/bin\\SubsetML_debug_spareDate100_dayLag0.pkl\n",
      "{'Subset_params': {'idxLengthOneMonth': 21, 'fouriercutoff': 5, 'multFactor': 6, 'daysAfterPrediction': 21, 'monthsHorizon': 6, 'timesteps': 5, 'classificationInterval': [0.05], 'optuna_trials': 10, 'LGBM_max_depth': 10, 'averageOverDays': 5}}\n"
     ]
    }
   ],
   "source": [
    "loadup_name = \"SubsetML_debug_spareDate100_dayLag0\"\n",
    "subsetML.load_data('../src/predictionModule/bin', loadup_name)\n",
    "\n",
    "\n",
    "print(subsetML.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(172508, 1186)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsetML.X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "415\n"
     ]
    }
   ],
   "source": [
    "X_train = pd.DataFrame(subsetML.X_train, columns=subsetML.featureColumnNames)\n",
    "\n",
    "X_train.shape\n",
    "\n",
    "featCol = subsetML.featureColumnNames\n",
    "print(subsetML.featureColumnNames.index(\"Fourier_Price_RSME\"))\n",
    "print(subsetML.featureColumnNames.index(\"FinData_ann_reportedEPS\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Distribution:\n",
      "  Label 0: Count = 115889, Frequency = 0.67\n",
      "  Label 1: Count = 56619, Frequency = 0.33\n",
      "\n",
      "Validation Label Distribution:\n",
      "  Label 0: Count = 29016, Frequency = 0.67\n",
      "  Label 1: Count = 14111, Frequency = 0.33\n",
      "\n",
      "Testing Label Distribution:\n",
      "  Label 0: Count = 3196, Frequency = 0.72\n",
      "  Label 1: Count = 1220, Frequency = 0.28\n",
      "\n",
      "Training Open Description:\n",
      "count    172508.000000\n",
      "mean          0.999952\n",
      "std           0.018741\n",
      "min           0.727763\n",
      "25%           0.990249\n",
      "50%           0.999698\n",
      "75%           1.009352\n",
      "max           1.704104\n",
      "Name: FeatureTA_Open, dtype: float64\n",
      "Testing Open Description:\n",
      "count    4416.000000\n",
      "mean        0.998606\n",
      "std         0.014713\n",
      "min         0.832682\n",
      "25%         0.990621\n",
      "50%         0.998316\n",
      "75%         1.006142\n",
      "max         1.096435\n",
      "Name: FeatureTA_Open, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import optuna\n",
    "X_train = pd.DataFrame(subsetML.X_train, columns=subsetML.featureColumnNames)\n",
    "X_test = pd.DataFrame(subsetML.X_test, columns=subsetML.featureColumnNames)\n",
    "y_train = subsetML.y_train\n",
    "y_test = subsetML.y_test\n",
    "X_val = pd.DataFrame(subsetML.X_val, columns=subsetML.featureColumnNames)\n",
    "y_val = subsetML.y_val\n",
    "colNames = subsetML.featureColumnNames\n",
    "\n",
    "print(\"Training Label Distribution:\")\n",
    "ModelAnalyzer().print_label_distribution(y_train)\n",
    "print(\"Validation Label Distribution:\")\n",
    "ModelAnalyzer().print_label_distribution(y_val)\n",
    "print(\"Testing Label Distribution:\")\n",
    "ModelAnalyzer().print_label_distribution(y_test)\n",
    "\n",
    "print(\"Training Open Description:\")\n",
    "print(X_train[\"FeatureTA_Open\"].describe())\n",
    "\n",
    "print(\"Testing Open Description:\")\n",
    "print(X_test[\"FeatureTA_Open\"].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 21:57:37,841] A new study created in memory with name: no-name-e6e30bdf-1669-4f12-8b20-d671c09098ba\n",
      "[I 2025-01-11 21:57:53,279] Trial 0 finished with value: 1.80207919174917 and parameters: {'feature_fraction': 0.00964565408341876, 'num_leaves': 236, 'max_depth': 7, 'learning_rate': 0.1393003111577421}. Best is trial 0 with value: 1.80207919174917.\n",
      "[I 2025-01-11 21:58:07,560] Trial 1 finished with value: 1.4606926307444206 and parameters: {'feature_fraction': 0.009304944660567017, 'num_leaves': 217, 'max_depth': 5, 'learning_rate': 0.010003134445663326}. Best is trial 0 with value: 1.80207919174917.\n",
      "[I 2025-01-11 21:58:25,424] Trial 2 finished with value: 1.8212693376749787 and parameters: {'feature_fraction': 0.0700941999833709, 'num_leaves': 115, 'max_depth': 7, 'learning_rate': 0.10063789483503538}. Best is trial 2 with value: 1.8212693376749787.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  feature_fraction: 0.0700941999833709\n",
      "  num_leaves: 115\n",
      "  max_depth: 7\n",
      "  learning_rate: 0.10063789483503538\n",
      "Validation accuracy: 0.9119808936397152\n",
      "Validation loss: 0.2401235669695423\n",
      "\n",
      "  Overall Accuracy: 0.91\n",
      "  Log Loss: 0.2401\n",
      "\n",
      "  Metrics per Class:\n",
      "    Class 0:\n",
      "      TPR: 0.91, FPR: 0.09, TNR: 0.91, FNR: 0.09\n",
      "    Class 1:\n",
      "      TPR: 0.91, FPR: 0.09, TNR: 0.91, FNR: 0.09\n",
      "\n",
      "test accuracy: 0.626358695652174\n",
      "test loss: 0.8965110141687969\n",
      "\n",
      "  Overall Accuracy: 0.63\n",
      "  Log Loss: 0.8965\n",
      "\n",
      "  Metrics per Class:\n",
      "    Class 0:\n",
      "      TPR: 0.79, FPR: 0.81, TNR: 0.19, FNR: 0.21\n",
      "    Class 1:\n",
      "      TPR: 0.19, FPR: 0.21, TNR: 0.79, FNR: 0.81\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define XGBoost parameters if not provided\n",
    "def objective(trial):\n",
    "    lgbm_params = {\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1,\n",
    "        'is_unbalance': True,\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': 1,\n",
    "        'lambda_l2': 1,\n",
    "        'n_estimators': 500,\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.005, 0.1, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 16, 256),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 8),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.005, 0.2, log=True),\n",
    "    }\n",
    "    # Initialize and train LGBM model\n",
    "    LGBMModel = lgb.LGBMClassifier(**lgbm_params)\n",
    "    LGBMModel.fit(X_train, y_train,\n",
    "                    eval_set=[(subsetML.X_val, subsetML.y_val)])\n",
    "    y_val_pred = LGBMModel.predict(X_val)\n",
    "    cm:np.array = confusion_matrix(subsetML.y_val, y_val_pred, labels=np.unique(subsetML.y_val))\n",
    "    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    return np.sum(per_class_accuracy)\n",
    "\n",
    "# 3. Create a study object and optimize the objective function.\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials = 3, show_progress_bar=False)\n",
    "\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "lgbm_params = {\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "    'is_unbalance': True,\n",
    "    'metric': 'binary_logloss',\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'n_estimators': 500,\n",
    "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
    "    'num_leaves': study.best_trial.params['num_leaves'],\n",
    "    'max_depth': study.best_trial.params['max_depth'],\n",
    "    'learning_rate': study.best_trial.params['learning_rate'],\n",
    "}\n",
    "# Initialize and train LGBM model\n",
    "LGBMModel = lgb.LGBMClassifier(**lgbm_params)\n",
    "LGBMModel.fit(X_train, y_train,\n",
    "                eval_set=[(subsetML.X_val, subsetML.y_val)])\n",
    "\n",
    "y_pred_val = LGBMModel.predict(X_val)\n",
    "y_pred_proba_val = LGBMModel.predict_proba(X_val)\n",
    "val_acc = accuracy_score(y_val, y_pred_val)\n",
    "val_loss = log_loss(y_val, y_pred_proba_val)\n",
    "\n",
    "print(f\"Validation accuracy: {val_acc}\")\n",
    "print(f\"Validation loss: {val_loss}\")\n",
    "\n",
    "ModelAnalyzer().print_classification_metrics(y_val, y_pred_val, y_pred_proba_val)\n",
    "\n",
    "y_pred_test = LGBMModel.predict(X_test)\n",
    "y_pred_proba_test = LGBMModel.predict_proba(X_test)\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "test_loss = log_loss(y_test, y_pred_proba_test)\n",
    "\n",
    "print(f\"test accuracy: {test_acc}\")\n",
    "print(f\"test loss: {test_loss}\")\n",
    "\n",
    "ModelAnalyzer().print_classification_metrics(y_test, y_pred_test, y_pred_proba_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201\n",
      "72\n",
      "Fourier_Price_RSME\n",
      "shape: (9, 2)\n",
      "┌────────────┬──────────┐\n",
      "│ statistic  ┆ column_0 │\n",
      "│ ---        ┆ ---      │\n",
      "│ str        ┆ f64      │\n",
      "╞════════════╪══════════╡\n",
      "│ count      ┆ 172508.0 │\n",
      "│ null_count ┆ 0.0      │\n",
      "│ mean       ┆ 0.026054 │\n",
      "│ std        ┆ 0.022904 │\n",
      "│ min        ┆ 0.001329 │\n",
      "│ 25%        ┆ 0.012946 │\n",
      "│ 50%        ┆ 0.019037 │\n",
      "│ 75%        ┆ 0.030017 │\n",
      "│ max        ┆ 0.280279 │\n",
      "└────────────┴──────────┘\n",
      "shape: (9, 2)\n",
      "┌────────────┬──────────┐\n",
      "│ statistic  ┆ column_0 │\n",
      "│ ---        ┆ ---      │\n",
      "│ str        ┆ f64      │\n",
      "╞════════════╪══════════╡\n",
      "│ count      ┆ 4416.0   │\n",
      "│ null_count ┆ 0.0      │\n",
      "│ mean       ┆ 0.02319  │\n",
      "│ std        ┆ 0.021151 │\n",
      "│ min        ┆ 0.001512 │\n",
      "│ 25%        ┆ 0.011492 │\n",
      "│ 50%        ┆ 0.016934 │\n",
      "│ 75%        ┆ 0.026965 │\n",
      "│ max        ┆ 0.339561 │\n",
      "└────────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(colNames.index('FinData_quar_surprise'))\n",
    "colNames.index(\"FinData_quar_surprise\")\n",
    "colNames.index(\"Fourier_Price_RSME\")\n",
    "columnToSubset = colNames.index(\"Fourier_Price_RSME\") #np.random.randint(0, len(colNames))\n",
    "print(columnToSubset)\n",
    "mask_quantile = np.quantile(subsetML.X_test[:, columnToSubset], 0.05)\n",
    "print(colNames[columnToSubset])\n",
    "print(pl.DataFrame(subsetML.X_train[:, columnToSubset]).describe())\n",
    "print(pl.DataFrame(subsetML.X_test[:, columnToSubset]).describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = subsetML.X_train[:, columnToSubset] < mask_quantile\n",
    "mask_X_Train = subsetML.X_train[mask,:]\n",
    "mask_y_Train = subsetML.y_train[mask]\n",
    "\n",
    "mask = subsetML.X_val[:,columnToSubset] < mask_quantile\n",
    "mask_X_val = subsetML.X_val[mask,:]\n",
    "mask_y_val = subsetML.y_val[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-01-11 23:31:45,102] A new study created in memory with name: no-name-3221d46d-a8fb-4562-901a-ac29a85494a7\n",
      "[I 2025-01-11 23:31:45,840] Trial 0 finished with value: 1.6219495813370348 and parameters: {'feature_fraction': 0.0018414972623986723, 'num_leaves': 444, 'max_depth': 2, 'learning_rate': 0.07148764578563337}. Best is trial 0 with value: 1.6219495813370348.\n",
      "[I 2025-01-11 23:31:47,043] Trial 1 finished with value: 1.8085585663161448 and parameters: {'feature_fraction': 0.05174191069191712, 'num_leaves': 297, 'max_depth': 10, 'learning_rate': 0.1328220304720362}. Best is trial 1 with value: 1.8085585663161448.\n",
      "[I 2025-01-11 23:31:48,004] Trial 2 finished with value: 1.8059890675750614 and parameters: {'feature_fraction': 0.007905440634946677, 'num_leaves': 157, 'max_depth': 9, 'learning_rate': 0.10892410062605648}. Best is trial 1 with value: 1.8085585663161448.\n",
      "[I 2025-01-11 23:31:49,341] Trial 3 finished with value: 1.6160975996197418 and parameters: {'feature_fraction': 0.0011396728906192676, 'num_leaves': 505, 'max_depth': 10, 'learning_rate': 0.10346310777717001}. Best is trial 1 with value: 1.8085585663161448.\n",
      "[I 2025-01-11 23:31:53,308] Trial 4 finished with value: 1.8170420454036629 and parameters: {'feature_fraction': 0.1464449649220293, 'num_leaves': 329, 'max_depth': 9, 'learning_rate': 0.010574765581525272}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:31:53,748] Trial 5 finished with value: 1.5653276110894883 and parameters: {'feature_fraction': 0.0013764660485897763, 'num_leaves': 102, 'max_depth': 2, 'learning_rate': 0.02984319976496572}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:31:55,121] Trial 6 finished with value: 1.8044391018465258 and parameters: {'feature_fraction': 0.05897923135212406, 'num_leaves': 394, 'max_depth': 6, 'learning_rate': 0.1321212683608873}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:31:55,947] Trial 7 finished with value: 1.7784168994485565 and parameters: {'feature_fraction': 0.019213403313184146, 'num_leaves': 440, 'max_depth': 4, 'learning_rate': 0.019446917896515453}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:31:56,988] Trial 8 finished with value: 1.8095884324335496 and parameters: {'feature_fraction': 0.022711550009218446, 'num_leaves': 141, 'max_depth': 8, 'learning_rate': 0.09251585288779732}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:31:58,583] Trial 9 finished with value: 1.7650596392368656 and parameters: {'feature_fraction': 0.0022976030067591593, 'num_leaves': 423, 'max_depth': 9, 'learning_rate': 0.012587357751471002}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:32:02,138] Trial 10 finished with value: 1.8108731818040913 and parameters: {'feature_fraction': 0.18504866540589443, 'num_leaves': 259, 'max_depth': 14, 'learning_rate': 0.04322087430186166}. Best is trial 4 with value: 1.8170420454036629.\n",
      "[I 2025-01-11 23:32:07,992] Trial 11 finished with value: 1.8301650885719303 and parameters: {'feature_fraction': 0.1971606539091168, 'num_leaves': 252, 'max_depth': 14, 'learning_rate': 0.010033974234421776}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:12,671] Trial 12 finished with value: 1.815757296033121 and parameters: {'feature_fraction': 0.1611270672257538, 'num_leaves': 287, 'max_depth': 14, 'learning_rate': 0.01316925665885964}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:15,711] Trial 13 finished with value: 1.8126676976809069 and parameters: {'feature_fraction': 0.07187817071801375, 'num_leaves': 234, 'max_depth': 12, 'learning_rate': 0.010308186226378918}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:18,058] Trial 14 finished with value: 1.8237310086143652 and parameters: {'feature_fraction': 0.10808532458213337, 'num_leaves': 354, 'max_depth': 7, 'learning_rate': 0.01981224827611857}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:19,011] Trial 15 finished with value: 1.8108525155943775 and parameters: {'feature_fraction': 0.007015277741177404, 'num_leaves': 346, 'max_depth': 6, 'learning_rate': 0.01960522781918167}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:20,632] Trial 16 finished with value: 1.8185816780273414 and parameters: {'feature_fraction': 0.09012065611395094, 'num_leaves': 215, 'max_depth': 6, 'learning_rate': 0.019308896116165597}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:22,530] Trial 17 finished with value: 1.8219364927375494 and parameters: {'feature_fraction': 0.03290207909053404, 'num_leaves': 192, 'max_depth': 12, 'learning_rate': 0.032762015007097016}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:26,326] Trial 18 finished with value: 1.827595589830847 and parameters: {'feature_fraction': 0.1134461078133739, 'num_leaves': 356, 'max_depth': 12, 'learning_rate': 0.01540770630124749}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:27,136] Trial 19 finished with value: 1.8095884324335496 and parameters: {'feature_fraction': 0.0386209673814827, 'num_leaves': 81, 'max_depth': 12, 'learning_rate': 0.19548583016577906}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:29,022] Trial 20 finished with value: 1.8260456241023115 and parameters: {'feature_fraction': 0.009531140297141956, 'num_leaves': 388, 'max_depth': 13, 'learning_rate': 0.015707358302342326}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:30,915] Trial 21 finished with value: 1.8237310086143652 and parameters: {'feature_fraction': 0.00954793552847165, 'num_leaves': 391, 'max_depth': 13, 'learning_rate': 0.014478051579082381}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:32,358] Trial 22 finished with value: 1.816277395644252 and parameters: {'feature_fraction': 0.004349126341558782, 'num_leaves': 304, 'max_depth': 11, 'learning_rate': 0.02706854952675036}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:34,443] Trial 23 finished with value: 1.7967202725184186 and parameters: {'feature_fraction': 0.004049030656390843, 'num_leaves': 493, 'max_depth': 14, 'learning_rate': 0.01539039396140844}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:36,280] Trial 24 finished with value: 1.824251108225496 and parameters: {'feature_fraction': 0.013054791507009993, 'num_leaves': 378, 'max_depth': 13, 'learning_rate': 0.025530681783688322}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:38,369] Trial 25 finished with value: 1.8173072617616566 and parameters: {'feature_fraction': 0.11129280882765596, 'num_leaves': 276, 'max_depth': 13, 'learning_rate': 0.05987495266853213}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:40,163] Trial 26 finished with value: 1.8021244863585792 and parameters: {'feature_fraction': 0.029793551892699017, 'num_leaves': 334, 'max_depth': 11, 'learning_rate': 0.04131121046838009}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:42,152] Trial 27 finished with value: 1.7936306741662045 and parameters: {'feature_fraction': 0.004380471156104108, 'num_leaves': 466, 'max_depth': 11, 'learning_rate': 0.015577626984307952}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:47,306] Trial 28 finished with value: 1.8124128144277698 and parameters: {'feature_fraction': 0.1993786932192103, 'num_leaves': 256, 'max_depth': 13, 'learning_rate': 0.010429096182744445}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:49,285] Trial 29 finished with value: 1.8095884324335496 and parameters: {'feature_fraction': 0.014545643295581525, 'num_leaves': 411, 'max_depth': 14, 'learning_rate': 0.055917509500402175}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:50,986] Trial 30 finished with value: 1.7779381322568535 and parameters: {'feature_fraction': 0.0027708483097482993, 'num_leaves': 464, 'max_depth': 12, 'learning_rate': 0.022716994762258923}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:52,887] Trial 31 finished with value: 1.8093232160755557 and parameters: {'feature_fraction': 0.011330721361726666, 'num_leaves': 364, 'max_depth': 13, 'learning_rate': 0.016450010025559226}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:54,825] Trial 32 finished with value: 1.8098433156866864 and parameters: {'feature_fraction': 0.01584781328022476, 'num_leaves': 314, 'max_depth': 13, 'learning_rate': 0.024092902733761974}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:56,341] Trial 33 finished with value: 1.8062439508281982 and parameters: {'feature_fraction': 0.006376377967518287, 'num_leaves': 388, 'max_depth': 10, 'learning_rate': 0.03625470385499682}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:32:59,509] Trial 34 finished with value: 1.8257907408491745 and parameters: {'feature_fraction': 0.049578581116077394, 'num_leaves': 373, 'max_depth': 14, 'learning_rate': 0.012728568882036951}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:02,362] Trial 35 finished with value: 1.8188468943853353 and parameters: {'feature_fraction': 0.04692304993928967, 'num_leaves': 319, 'max_depth': 14, 'learning_rate': 0.01249371262415408}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:06,693] Trial 36 finished with value: 1.815757296033121 and parameters: {'feature_fraction': 0.13091161451569983, 'num_leaves': 426, 'max_depth': 11, 'learning_rate': 0.011608618479752773}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:09,612] Trial 37 finished with value: 1.81987676050274 and parameters: {'feature_fraction': 0.06821091703030976, 'num_leaves': 196, 'max_depth': 12, 'learning_rate': 0.01587886514283949}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:13,055] Trial 38 finished with value: 1.823476125361228 and parameters: {'feature_fraction': 0.08430293317270256, 'num_leaves': 364, 'max_depth': 14, 'learning_rate': 0.013326329897321632}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:13,881] Trial 39 finished with value: 1.5831005514433625 and parameters: {'feature_fraction': 0.13148532065150276, 'num_leaves': 404, 'max_depth': 2, 'learning_rate': 0.010171858843705849}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:16,177] Trial 40 finished with value: 1.8188468943853353 and parameters: {'feature_fraction': 0.055385560681572854, 'num_leaves': 146, 'max_depth': 9, 'learning_rate': 0.017574840501686728}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:18,315] Trial 41 finished with value: 1.8131877972920376 and parameters: {'feature_fraction': 0.024214426657843933, 'num_leaves': 378, 'max_depth': 13, 'learning_rate': 0.023347927405380914}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:20,356] Trial 42 finished with value: 1.8196115441447462 and parameters: {'feature_fraction': 0.009406174687388098, 'num_leaves': 452, 'max_depth': 13, 'learning_rate': 0.011579776822076188}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:22,358] Trial 43 finished with value: 1.803409235729121 and parameters: {'feature_fraction': 0.02140344507724316, 'num_leaves': 340, 'max_depth': 14, 'learning_rate': 0.028694213564616048}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:24,130] Trial 44 finished with value: 1.820131643755877 and parameters: {'feature_fraction': 0.005907167227589191, 'num_leaves': 372, 'max_depth': 12, 'learning_rate': 0.013985822963747972}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:28,373] Trial 45 finished with value: 1.8286254559482518 and parameters: {'feature_fraction': 0.15374722100748683, 'num_leaves': 326, 'max_depth': 10, 'learning_rate': 0.017560925901487015}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:32,577] Trial 46 finished with value: 1.8188468943853353 and parameters: {'feature_fraction': 0.15937774240226987, 'num_leaves': 284, 'max_depth': 10, 'learning_rate': 0.018052398571729217}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:35,402] Trial 47 finished with value: 1.8288803392013886 and parameters: {'feature_fraction': 0.09987753284765585, 'num_leaves': 252, 'max_depth': 8, 'learning_rate': 0.02132967573371209}. Best is trial 11 with value: 1.8301650885719303.\n",
      "[I 2025-01-11 23:33:38,263] Trial 48 finished with value: 1.8311949546893351 and parameters: {'feature_fraction': 0.09635713782427702, 'num_leaves': 249, 'max_depth': 8, 'learning_rate': 0.021342333878133737}. Best is trial 48 with value: 1.8311949546893351.\n",
      "[I 2025-01-11 23:33:41,019] Trial 49 finished with value: 1.8309400714361983 and parameters: {'feature_fraction': 0.10429394703074213, 'num_leaves': 229, 'max_depth': 8, 'learning_rate': 0.021824577580581213}. Best is trial 48 with value: 1.8311949546893351.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  feature_fraction: 0.09635713782427702\n",
      "  num_leaves: 249\n",
      "  max_depth: 8\n",
      "  learning_rate: 0.021342333878133737\n",
      "Validation accuracy: 0.9346456692913386\n",
      "Validation loss: 0.1594674643591774\n",
      "\n",
      "  Overall Accuracy: 0.93\n",
      "  Log Loss: 0.1595\n",
      "\n",
      "  Metrics per Class:\n",
      "    Class 0:\n",
      "      TPR: 0.95, FPR: 0.12, TNR: 0.88, FNR: 0.05\n",
      "    Class 1:\n",
      "      TPR: 0.88, FPR: 0.05, TNR: 0.95, FNR: 0.12\n",
      "\n"
     ]
    }
   ],
   "source": [
    "LGBMModel2 = lgb.LGBMClassifier()\n",
    "def objective(trial):\n",
    "    lgbm_params = {\n",
    "        'verbosity': -1,\n",
    "        'n_jobs': -1,\n",
    "        'is_unbalance': True,\n",
    "        'metric': 'binary_logloss',\n",
    "        'lambda_l1': 1,\n",
    "        'lambda_l2': 1,\n",
    "        'n_estimators': 500,\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.001, 0.2, log=True),\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 64, 512),\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 14),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01,0.2, log=True),\n",
    "    }\n",
    "    # Initialize and train LGBM model\n",
    "    LGBMModel2 = lgb.LGBMClassifier(**lgbm_params)\n",
    "    LGBMModel2.fit(mask_X_Train, mask_y_Train,\n",
    "                    eval_set=[(mask_X_val, mask_y_val)])\n",
    "    mask_y_val_pred = LGBMModel2.predict(mask_X_val)\n",
    "    cm:np.array = confusion_matrix(mask_y_val, mask_y_val_pred, labels=np.unique(mask_y_val))\n",
    "    per_class_accuracy = cm.diagonal() / cm.sum(axis=1)\n",
    "    return np.sum(per_class_accuracy)\n",
    "\n",
    "# 3. Create a study2 object and optimize the objective function.\n",
    "study2 = optuna.create_study(direction='maximize')\n",
    "study2.optimize(objective, n_trials = 50, show_progress_bar=False)\n",
    "\n",
    "for key, value in study2.best_trial.params.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "    \n",
    "lgbm_params = {\n",
    "    'verbosity': -1,\n",
    "    'n_jobs': -1,\n",
    "    'is_unbalance': True,\n",
    "    'metric': 'binary_logloss',\n",
    "    'lambda_l1': 1,\n",
    "    'lambda_l2': 1,\n",
    "    'n_estimators': 500,\n",
    "    'feature_fraction': study2.best_trial.params['feature_fraction'],\n",
    "    'num_leaves': study2.best_trial.params['num_leaves'],\n",
    "    'max_depth': study2.best_trial.params['max_depth'],\n",
    "    'learning_rate': study2.best_trial.params['learning_rate'],\n",
    "}\n",
    "# Initialize and train LGBM model\n",
    "LGBMModel2 = lgb.LGBMClassifier(**lgbm_params)\n",
    "LGBMModel2.fit(mask_X_Train, mask_y_Train,\n",
    "                eval_set=[(mask_X_val, mask_y_val)])\n",
    "\n",
    "mask_y_pred_val = LGBMModel2.predict(mask_X_val)\n",
    "mask_y_pred_proba_val = LGBMModel2.predict_proba(mask_X_val)\n",
    "mask_val_acc = accuracy_score(mask_y_val, mask_y_pred_val)\n",
    "mask_val_loss = log_loss(mask_y_val, mask_y_pred_proba_val)\n",
    "\n",
    "print(f\"Validation accuracy: {mask_val_acc}\")\n",
    "print(f\"Validation loss: {mask_val_loss}\")\n",
    "\n",
    "ModelAnalyzer().print_classification_metrics(mask_y_val, mask_y_pred_val, mask_y_pred_proba_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 Feature Importances:\n",
      "                                        Feature Importance\n",
      "Rank                                                      \n",
      "1         FeatureTA_momentum_pvo_signal_lag_m21   233.0000\n",
      "2            FeatureTA_trend_mass_index_lag_m21   168.0000\n",
      "3                  FeatureTA_volume_vpt_lag_m21   150.0000\n",
      "4                   FeatureTA_trend_adx_lag_m21   149.0000\n",
      "5               FeatureTA_trend_kst_sig_lag_m21   126.0000\n",
      "6                 Seasonal_week_of_year_lag_m21   121.0000\n",
      "7        FeatureTA_trend_vortex_ind_pos_lag_m21   117.0000\n",
      "8                FeatureTA_momentum_tsi_lag_m21   112.0000\n",
      "9                  FeatureTA_volume_cmf_lag_m21   111.0000\n",
      "10       FeatureTA_momentum_stoch_rsi_k_lag_m21   108.0000\n",
      "11              FeatureTA_volatility_ui_lag_m10   104.0000\n",
      "12    FeatureTA_trend_visual_ichimoku_b_lag_m21    94.0000\n",
      "13              FeatureTA_volatility_ui_lag_m21    94.0000\n",
      "14             Fourier_Price_lag_m10_AbsCoeff_2    94.0000\n",
      "15              Fourier_Price_lag_m3_AbsCoeff_1    92.0000\n",
      "16                 FeatureTA_volume_obv_lag_m21    92.0000\n",
      "17                     FeatureTA_volatility_atr    92.0000\n",
      "18              Fourier_Price_lag_m3_AbsCoeff_4    91.0000\n",
      "19    FeatureTA_trend_visual_ichimoku_b_lag_m10    91.0000\n",
      "20         FeatureTA_momentum_pvo_signal_lag_m1    90.0000\n"
     ]
    }
   ],
   "source": [
    "ModelAnalyzer().print_feature_importance_LGBM(LGBMModel2, colNames,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.5746606334841629\n",
      "Validation loss: 1.1236485784793753\n",
      "\n",
      "  Overall Accuracy: 0.57\n",
      "  Log Loss: 1.1236\n",
      "\n",
      "  Metrics per Class:\n",
      "    Class 0:\n",
      "      TPR: 1.00, FPR: 0.98, TNR: 0.02, FNR: 0.00\n",
      "    Class 1:\n",
      "      TPR: 0.02, FPR: 0.00, TNR: 1.00, FNR: 0.98\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mask = subsetML.X_test[:,columnToSubset] < mask_quantile\n",
    "mask_X_test = subsetML.X_test[mask,:]\n",
    "mask_y_test = subsetML.y_test[mask]\n",
    "\n",
    "mask_y_pred_test = LGBMModel2.predict(mask_X_test)\n",
    "mask_y_pred_proba_test = LGBMModel2.predict_proba(mask_X_test)\n",
    "mask_test_acc = accuracy_score(mask_y_test, mask_y_pred_test)\n",
    "mask_test_loss = log_loss(mask_y_test, mask_y_pred_proba_test)\n",
    "\n",
    "print(f\"Validation accuracy: {mask_test_acc}\")\n",
    "print(f\"Validation loss: {mask_test_loss}\")\n",
    "\n",
    "ModelAnalyzer().print_classification_metrics(mask_y_test, mask_y_pred_test, mask_y_pred_proba_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
