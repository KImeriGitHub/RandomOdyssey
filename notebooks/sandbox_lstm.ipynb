{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c3b363e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Input, GaussianNoise, LSTM, Bidirectional, Dropout, Dense, Conv1D\n",
    "from tensorflow.keras import regularizers, backend as K\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.metrics import RootMeanSquaredError, R2Score\n",
    "\n",
    "# Get the absolute path to the project directory\n",
    "project_dir = os.path.abspath(\"..\")\n",
    "\n",
    "# Append the project directory to sys.path\n",
    "if project_dir not in sys.path:\n",
    "    sys.path.append(project_dir)\n",
    "    \n",
    "from src.predictionModule.TreeTimeML_save import TreeTimeML\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb954e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"daysAfterPrediction\": 5,\n",
    "    'timesteps': 25,\n",
    "    'target_option': 'last',\n",
    "    \"TreeTime_isFiltered\": True,\n",
    "    \"TreeTime_volatility_atr_qup\": 0.90,\n",
    "    \"TreeTime_top_highest\": 10,\n",
    "    \n",
    "    \"TreeTime_lstm_units\": 64,\n",
    "    \"TreeTime_lstm_num_layers\": 3,\n",
    "    \"TreeTime_lstm_dropout\": 0.00001,\n",
    "    \"TreeTime_lstm_recurrent_dropout\": 0.00001,\n",
    "    \"TreeTime_lstm_learning_rate\": 0.001,\n",
    "    \"TreeTime_lstm_optimizer\": \"adam\",\n",
    "    \"TreeTime_lstm_bidirectional\": True,\n",
    "    \"TreeTime_lstm_batch_size\": 2**14,\n",
    "    \"TreeTime_lstm_epochs\": 4,\n",
    "    \"TreeTime_lstm_l1\": 0.00001,\n",
    "    \"TreeTime_lstm_l2\": 0.00001,\n",
    "    \"TreeTime_inter_dropout\": 0.00001,\n",
    "    \"TreeTime_input_gaussian_noise\": 0.00001,\n",
    "    \"TreeTime_lstm_conv1d\": True,\n",
    "    \"TreeTime_lstm_conv1d_kernel_size\": 3,\n",
    "    \"TreeTime_lstm_loss\": \"mse\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c306cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_group = \"group_snp500_finanTo2011\"\n",
    "\n",
    "eval_date = datetime.date(year=2025, month=2, day=13)\n",
    "start_train_date = datetime.date(year=2014, month=1, day=1)\n",
    "\n",
    "treetimeML = TreeTimeML(\n",
    "    train_start_date=start_train_date,\n",
    "    test_dates=[eval_date],\n",
    "    group=stock_group,\n",
    "    params=params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a74d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "treetimeML.load_and_filter_sets(main_path = \"../src/featureAlchemy/bin/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21232b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Xtree = treetimeML.train_Xtree\n",
    "train_ytree = treetimeML.train_ytree\n",
    "train_Xtime = treetimeML.train_Xtime\n",
    "train_ytime = treetimeML.train_ytime\n",
    "\n",
    "test_Xtree = treetimeML.test_Xtree\n",
    "test_ytree = treetimeML.test_ytree\n",
    "test_Xtime = treetimeML.test_Xtime\n",
    "test_ytime = treetimeML.test_ytime\n",
    "\n",
    "treenames = treetimeML.featureTreeNames\n",
    "timenames = treetimeML.featureTimeNames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80dc05d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = np.where(timenames == \"MathFeature_TradedPrice\")[0][0]\n",
    "idx2 = np.where(timenames == \"FeatureTA_High\")[0][0]\n",
    "idx3 = np.where(timenames == \"FeatureTA_Low\")[0][0]\n",
    "idx4 = np.where(timenames == \"FeatureTA_volume_obv\")[0][0]\n",
    "\n",
    "train_Xtime = train_Xtime[:, :, [idx1, idx2, idx3, idx4]]\n",
    "test_Xtime = test_Xtime[:, :, [idx1, idx2, idx3, idx4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea6d76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(params, train_Xtime=train_Xtime, train_ytime=train_ytime, training_ratio=0.95):\n",
    "    # Hyperparameters to tune\n",
    "    lstm_units = params[\"TreeTime_lstm_units\"]\n",
    "    num_layers = params[\"TreeTime_lstm_num_layers\"]\n",
    "    dropout = params[\"TreeTime_lstm_dropout\"]\n",
    "    recurrent_dropout = params[\"TreeTime_lstm_recurrent_dropout\"]\n",
    "    learning_rate = params[\"TreeTime_lstm_learning_rate\"]\n",
    "    optimizer_name = params[\"TreeTime_lstm_optimizer\"]\n",
    "    bidirectional = params[\"TreeTime_lstm_bidirectional\"]\n",
    "    batch_size = params[\"TreeTime_lstm_batch_size\"]\n",
    "    epochs = params[\"TreeTime_lstm_epochs\"]\n",
    "    loss_name = params[\"TreeTime_lstm_loss\"]\n",
    "\n",
    "    # Regularization hyperparameters\n",
    "    l1 = params.get(\"TreeTime_lstm_l1\", 0.0)\n",
    "    l2 = params.get(\"TreeTime_lstm_l2\", 0.0)\n",
    "    inter_dropout = params.get(\"TreeTime_inter_dropout\", 0.0)\n",
    "    noise_std = params.get(\"TreeTime_input_gaussian_noise\", 0.0)\n",
    "\n",
    "    # Conv1D option\n",
    "    use_conv1d = params.get(\"TreeTime_lstm_conv1d\", False)\n",
    "    conv_filters = lstm_units\n",
    "    conv_kernel = params.get(\"TreeTime_lstm_conv1d_kernel_size\", 3)\n",
    "    X_full, y_full = train_Xtime, train_ytime\n",
    "    n_total = X_full.shape[0]\n",
    "    split_at = int(n_total * training_ratio)\n",
    "    X_train, X_holdout = X_full[:split_at], X_full[split_at:]\n",
    "    y_train, y_holdout = y_full[:split_at], y_full[split_at:]\n",
    "\n",
    "    # Build model\n",
    "    model = Sequential([Input(shape=train_Xtime.shape[1:])])\n",
    "    # Add Gaussian noise to inputs\n",
    "    if noise_std > 0:\n",
    "        model.add(GaussianNoise(noise_std))\n",
    "    # Add Conv1D layer if opted in\n",
    "    if use_conv1d:\n",
    "        model.add(Conv1D(filters=conv_filters,\n",
    "                        kernel_size=conv_kernel,\n",
    "                        padding='same',\n",
    "                        activation='linear'))\n",
    "    # Add LSTM layers with regularization and dropout\n",
    "    for i in range(num_layers):\n",
    "        return_seq = i < (num_layers - 1)\n",
    "        lstm_layer = LSTM(\n",
    "            lstm_units,\n",
    "            return_sequences=return_seq,\n",
    "            dropout=dropout,\n",
    "            recurrent_dropout=recurrent_dropout,\n",
    "            kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)\n",
    "        )\n",
    "        if bidirectional:\n",
    "            model.add(Bidirectional(lstm_layer))\n",
    "        else:\n",
    "            model.add(lstm_layer)\n",
    "        # Add dropout between layers\n",
    "        if inter_dropout > 0 and return_seq:\n",
    "            model.add(Dropout(inter_dropout))\n",
    "    # Output layer\n",
    "    model.add(Dense(1, activation='linear', kernel_regularizer=regularizers.L1L2(l1=l1, l2=l2)))\n",
    "    # Optimizer\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = Adam(learning_rate=learning_rate)\n",
    "    elif optimizer_name == \"rmsprop\":\n",
    "        optimizer = RMSprop(learning_rate=learning_rate)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimizer: {optimizer_name}\")\n",
    "    def quantile_loss(q):\n",
    "        def loss(y_true, y_pred):\n",
    "            e = y_true - y_pred\n",
    "            return tf.reduce_mean(tf.maximum(q*e, (q-1)*e))\n",
    "        return loss\n",
    "    def r2_keras(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Returns R^2 metric: 1 - SS_res / SS_tot\n",
    "        \"\"\"\n",
    "        ss_res =  K.sum(K.square(y_true - y_pred)) \n",
    "        ss_tot = K.sum(K.square(y_true - K.mean(y_true)))\n",
    "        # avoid division by zero\n",
    "        return 1 - ss_res/(ss_tot + K.epsilon())\n",
    "    def neg_r2_loss(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Loss function to *maximize* R^2 by minimizing its negative.\n",
    "        \"\"\"\n",
    "        return -r2_keras(y_true, y_pred)\n",
    "    if loss_name == \"mse\":\n",
    "        loss_lstm = MeanSquaredError()\n",
    "    elif loss_name == \"r2\":\n",
    "        loss_lstm = neg_r2_loss\n",
    "    else:\n",
    "        # handles quantile_1,3,5,7,9 etc.\n",
    "        q = int(loss_name.split(\"_\")[1]) / 10.0\n",
    "        loss_lstm = quantile_loss(q)\n",
    "    # Compile\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=loss_lstm,\n",
    "        metrics=[MeanSquaredError(name='mse'),\n",
    "                RootMeanSquaredError(name='rmse')]\n",
    "    )\n",
    "    # Callbacks\n",
    "    class TimeLimit(Callback):\n",
    "        def __init__(self, max_seconds): super().__init__(); self.max_seconds = max_seconds\n",
    "        def on_train_begin(self, logs=None): self.t0 = time.time()\n",
    "        def on_batch_end(self, batch, logs=None): (time.time() - self.t0 > self.max_seconds) and setattr(self.model, 'stop_training', True)\n",
    "    es = EarlyStopping(monitor='loss', patience=3, restore_best_weights=True)\n",
    "    rlrop = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=2)\n",
    "    time_cb = TimeLimit(3600) \n",
    "    # Train\n",
    "    history = model.fit(\n",
    "        X_train,\n",
    "        y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=epochs,\n",
    "        validation_data=(X_holdout, y_holdout),\n",
    "        callbacks=[es, rlrop, time_cb],\n",
    "        shuffle=False,\n",
    "    )\n",
    "    \n",
    "    return min(history.history['val_rmse']), model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6636c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def run_gpt(params,\n",
    "        train_Xtime_np,\n",
    "        train_ytime_np,\n",
    "        training_ratio=0.95,\n",
    "        subset_ratio=0.1,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    \"\"\"\n",
    "    Train an LSTM on the first `training_ratio` of (train_Xtime, train_ytime),\n",
    "    then select the top `subset_ratio` fraction of samples by predicted score\n",
    "    on both train and held‐out validation, and report their actual means.\n",
    "    \"\"\"\n",
    "    # -- split train / val by time order --\n",
    "    train_Xtime = torch.from_numpy(train_Xtime_np).float()\n",
    "    train_ytime = torch.from_numpy(train_ytime_np).float()\n",
    "    N = train_Xtime.shape[0]\n",
    "    split = int(N * training_ratio)\n",
    "    X_train = train_Xtime[:split].to(device)\n",
    "    y_train = train_ytime[:split].to(device)\n",
    "    X_val   = train_Xtime[split:].to(device)\n",
    "    y_val   = train_ytime[split:].to(device)\n",
    "\n",
    "    # -- model setup --\n",
    "    D = X_train.size(-1)\n",
    "    H = params.get('hidden_size', 64)\n",
    "    model = nn.Sequential(\n",
    "        nn.LSTM(input_size=D, hidden_size=H, batch_first=True),\n",
    "        nn.Flatten(start_dim=0, end_dim=1),                     # (h, _) → (N, H)\n",
    "        nn.Linear(H, 1)\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=params.get('lr', 1e-3))\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # -- training loop --\n",
    "    batch_size = params.get('batch_size', 512)\n",
    "    train_ds = TensorDataset(X_train, y_train)\n",
    "    loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    for epoch in range(params.get('epochs', 10)):\n",
    "        model.train()\n",
    "        for xb, yb in loader:\n",
    "            optimizer.zero_grad()\n",
    "            preds, _ = model[0](xb)        # unpack LSTM\n",
    "            last = preds[:, -1, :]\n",
    "            out = model[2](last)\n",
    "            loss = loss_fn(out.squeeze(), yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # -- scoring and subset selection --\n",
    "    device = torch.device('cpu')\n",
    "    # move model parts\n",
    "    model0 = model[0].to(device)\n",
    "    model2 = model[2].to(device)\n",
    "\n",
    "    # move your data\n",
    "    X_train = X_train.to(device)\n",
    "    y_train = y_train.to(device)\n",
    "    X_val   = X_val.to(device)\n",
    "    y_val   = y_val.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # train\n",
    "        preds_tr, _ = model0(X_train)\n",
    "        scores_tr = model2(preds_tr[:, -1, :]).squeeze()\n",
    "        k_tr = max(1, int(len(scores_tr) * subset_ratio))\n",
    "        top_tr = torch.topk(scores_tr, k_tr).indices\n",
    "        mean_tr_subset = y_train[top_tr].mean().item()\n",
    "        mean_tr_all    = y_train.mean().item()\n",
    "\n",
    "        # validation\n",
    "        preds_val, _ = model0(X_val)\n",
    "        scores_val = model2(preds_val[:, -1, :]).squeeze()\n",
    "        k_val = max(1, int(len(scores_val) * subset_ratio))\n",
    "        top_val = torch.topk(scores_val, k_val).indices\n",
    "        mean_val_subset = y_val[top_val].mean().item()\n",
    "        mean_val_all    = y_val.mean().item()\n",
    "\n",
    "    # -- results --\n",
    "    torch.cuda.empty_cache()\n",
    "    return {\n",
    "        'mean_train_all': mean_tr_all,\n",
    "        'mean_train_subset': mean_tr_subset,\n",
    "        'mean_val_all': mean_val_all,\n",
    "        'mean_val_subset': mean_val_subset,\n",
    "        'train_subset_idx': top_tr.cpu().numpy(),\n",
    "        'val_subset_idx': top_val.cpu().numpy(),\n",
    "        'model': model\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "959f459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n",
      "True\n",
      "NVIDIA GeForce RTX 3060\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)\n",
    "print(torch.cuda.is_available())      # should print True\n",
    "print(torch.cuda.get_device_name(0))  # your GPU name\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86a5a283",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean y (all train):   0.533\n",
      "Mean y (top 10% train):0.616\n",
      "Mean y (all val):     0.522\n",
      "Mean y (top 10% val): 0.527\n",
      "Selected train indices: [37726 40720 40654 ... 45491 17313 50097]\n",
      "Selected val   indices: [2993 5093 5617 ... 1115  367 1676]\n"
     ]
    }
   ],
   "source": [
    "# set your LSTM params\n",
    "params = {\n",
    "    'hidden_size': 128,\n",
    "    'lr': 1e-3,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 50\n",
    "}\n",
    "\n",
    "results = run_gpt(params, train_Xtime, train_ytime,\n",
    "              training_ratio=0.90, subset_ratio=0.2)\n",
    "\n",
    "# inspect outcome\n",
    "print(f\"Mean y (all train):   {results['mean_train_all']:.3f}\")\n",
    "print(f\"Mean y (top 10% train):{results['mean_train_subset']:.3f}\")\n",
    "print(f\"Mean y (all val):     {results['mean_val_all']:.3f}\")\n",
    "print(f\"Mean y (top 10% val): {results['mean_val_subset']:.3f}\")\n",
    "\n",
    "# indices of selected samples\n",
    "print(\"Selected train indices:\", results['train_subset_idx'])\n",
    "print(\"Selected val   indices:\", results['val_subset_idx'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b44275dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = results['val_subset_idx']\n",
    "ddiff = np.diff(np.sort(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c2a0a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "params.update({\n",
    "    \"TreeTime_lstm_units\": 16,\n",
    "    \"TreeTime_lstm_num_layers\": 4,\n",
    "    \"TreeTime_lstm_learning_rate\": 0.002,\n",
    "    \"TreeTime_lstm_conv1d\": True,\n",
    "    \"TreeTime_lstm_batch_size\": 2**11,\n",
    "    \"TreeTime_lstm_epochs\": 2,\n",
    "    \"TreeTime_lstm_dropout\": 0.00,\n",
    "    \"TreeTime_inter_dropout\": 0.00,\n",
    "    \"TreeTime_input_gaussian_noise\": 0.0,\n",
    "    \"TreeTime_lstm_loss\": \"mse\",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1ebd8141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1/5\n",
      "Epoch 1/4\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 8s/step - loss: 0.3672 - mse: 0.2512 - rmse: 0.4989 - val_loss: 0.1738 - val_mse: 0.0611 - val_rmse: 0.2472 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8s/step - loss: 0.1861 - mse: 0.0744 - rmse: 0.2705 - val_loss: 0.1918 - val_mse: 0.0798 - val_rmse: 0.2825 - learning_rate: 0.0010\n",
      "Epoch 3/4\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8s/step - loss: 0.1859 - mse: 0.0738 - rmse: 0.2723 - val_loss: 0.1674 - val_mse: 0.0564 - val_rmse: 0.2375 - learning_rate: 0.0010\n",
      "Epoch 4/4\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 8s/step - loss: 0.1759 - mse: 0.0650 - rmse: 0.2548 - val_loss: 0.1640 - val_mse: 0.0540 - val_rmse: 0.2323 - learning_rate: 0.0010\n",
      "Validation RMSE: 0.2323\n",
      "\u001b[1m5/5\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2s/step\n",
      "Train RMSE: 0.2496\n",
      "Sqrt Variance: 0.0849\n",
      "mean y time: 0.5315\n",
      "pred Quantile 0.5: 0.5364\n",
      "pred Quantile 0.8: 0.5324\n",
      "Mask size: 0.7999969742356162\n",
      "Iteration 2/5\n",
      "Epoch 1/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 9s/step - loss: 0.3494 - mse: 0.2350 - rmse: 0.4811 - val_loss: 0.2258 - val_mse: 0.1128 - val_rmse: 0.3358 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9s/step - loss: 0.2132 - mse: 0.0999 - rmse: 0.3163 - val_loss: 0.1762 - val_mse: 0.0639 - val_rmse: 0.2527 - learning_rate: 0.0010\n",
      "Epoch 3/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 9s/step - loss: 0.1903 - mse: 0.0784 - rmse: 0.2790 - val_loss: 0.1816 - val_mse: 0.0700 - val_rmse: 0.2647 - learning_rate: 0.0010\n",
      "Epoch 4/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 8s/step - loss: 0.1826 - mse: 0.0711 - rmse: 0.2668 - val_loss: 0.1573 - val_mse: 0.0466 - val_rmse: 0.2158 - learning_rate: 0.0010\n",
      "Validation RMSE: 0.2158\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2s/step\n",
      "Train RMSE: 0.2396\n",
      "Sqrt Variance: 0.0589\n",
      "mean y time: 0.5339\n",
      "pred Quantile 0.5: 0.5376\n",
      "pred Quantile 0.8: 0.5335\n",
      "Mask size: 0.6399945536241093\n",
      "Iteration 3/5\n",
      "Epoch 1/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 6s/step - loss: 0.2755 - mse: 0.1581 - rmse: 0.4014 - val_loss: 0.1706 - val_mse: 0.0576 - val_rmse: 0.2401 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 0.1778 - mse: 0.0658 - rmse: 0.2547 - val_loss: 0.1847 - val_mse: 0.0726 - val_rmse: 0.2694 - learning_rate: 0.0010\n",
      "Epoch 3/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 0.1875 - mse: 0.0739 - rmse: 0.2748 - val_loss: 0.1574 - val_mse: 0.0463 - val_rmse: 0.2152 - learning_rate: 0.0010\n",
      "Epoch 4/4\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 6s/step - loss: 0.1669 - mse: 0.0562 - rmse: 0.2360 - val_loss: 0.1651 - val_mse: 0.0550 - val_rmse: 0.2345 - learning_rate: 0.0010\n",
      "Validation RMSE: 0.2152\n",
      "WARNING:tensorflow:5 out of the last 10 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B1402107C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m2/3\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2s/stepWARNING:tensorflow:6 out of the last 12 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001B1402107C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step\n",
      "Train RMSE: 0.2591\n",
      "Sqrt Variance: 0.0565\n",
      "mean y time: 0.5351\n",
      "pred Quantile 0.5: 0.5372\n",
      "pred Quantile 0.8: 0.5293\n",
      "Mask size: 0.51198959137052\n",
      "Iteration 4/5\n",
      "Epoch 1/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 9s/step - loss: 0.3823 - mse: 0.2622 - rmse: 0.5157 - val_loss: 0.2499 - val_mse: 0.1367 - val_rmse: 0.3698 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8s/step - loss: 0.2424 - mse: 0.1283 - rmse: 0.3593 - val_loss: 0.1603 - val_mse: 0.0475 - val_rmse: 0.2178 - learning_rate: 0.0010\n",
      "Epoch 3/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8s/step - loss: 0.1778 - mse: 0.0665 - rmse: 0.2536 - val_loss: 0.1991 - val_mse: 0.0867 - val_rmse: 0.2944 - learning_rate: 0.0010\n",
      "Epoch 4/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 8s/step - loss: 0.2025 - mse: 0.0906 - rmse: 0.3002 - val_loss: 0.1659 - val_mse: 0.0539 - val_rmse: 0.2322 - learning_rate: 0.0010\n",
      "Validation RMSE: 0.2178\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step\n",
      "Train RMSE: 0.3139\n",
      "Sqrt Variance: 0.0228\n",
      "mean y time: 0.5363\n",
      "pred Quantile 0.5: 0.5374\n",
      "pred Quantile 0.8: 0.5244\n",
      "Mask size: 0.4095825958032648\n",
      "Iteration 5/5\n",
      "Epoch 1/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 5s/step - loss: 0.5735 - mse: 0.4086 - rmse: 0.6766 - val_loss: 0.2706 - val_mse: 0.1576 - val_rmse: 0.3970 - learning_rate: 0.0010\n",
      "Epoch 2/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - loss: 0.2790 - mse: 0.1604 - rmse: 0.4073 - val_loss: 0.1807 - val_mse: 0.0679 - val_rmse: 0.2605 - learning_rate: 0.0010\n",
      "Epoch 3/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - loss: 0.1936 - mse: 0.0800 - rmse: 0.2843 - val_loss: 0.1795 - val_mse: 0.0669 - val_rmse: 0.2587 - learning_rate: 0.0010\n",
      "Epoch 4/4\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4s/step - loss: 0.1978 - mse: 0.0919 - rmse: 0.2916 - val_loss: 0.1863 - val_mse: 0.0740 - val_rmse: 0.2721 - learning_rate: 0.0010\n",
      "Validation RMSE: 0.2587\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step\n",
      "Train RMSE: 0.2879\n",
      "Sqrt Variance: 0.0215\n",
      "mean y time: 0.5370\n",
      "pred Quantile 0.5: 0.5298\n",
      "pred Quantile 0.8: 0.5255\n",
      "Mask size: 0.3276600251138444\n"
     ]
    }
   ],
   "source": [
    "q = 0.2\n",
    "itermax = 5\n",
    "\n",
    "mask = np.ones(train_Xtime.shape[0], dtype=bool)\n",
    "for i in range(itermax):\n",
    "    print(f\"Iteration {i+1}/{itermax}\")\n",
    "    \n",
    "    val_rmse, model = run(params, train_Xtime=train_Xtime[mask], train_ytime=train_ytime[mask], training_ratio=0.9)\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    \n",
    "    # Update mask based on validation RMSE\n",
    "    y_train_pred = model.predict(train_Xtime[mask], batch_size=params['TreeTime_lstm_batch_size'])[:,0]\n",
    "    rsme_train = np.sqrt(np.mean((y_train_pred - train_ytime[mask]) ** 2))\n",
    "    print(f\"Train RMSE: {rsme_train:.4f}\")\n",
    "    print(f\"Sqrt Variance: {np.sqrt(np.var(y_train_pred)):.4f}\")\n",
    "    print(f\"mean y time: {np.mean(train_ytime[mask]):.4f}\")\n",
    "    print(f\"pred Quantile 0.5: {np.mean(train_ytime[mask][y_train_pred > np.quantile(y_train_pred, 0.5)]):.4f}\")\n",
    "    print(f\"pred Quantile 0.8: {np.mean(train_ytime[mask][y_train_pred > np.quantile(y_train_pred, 0.8)]):.4f}\")\n",
    "    \n",
    "    mask_loop = (y_train_pred > np.quantile(y_train_pred, q))\n",
    "    prev_count = mask.sum()\n",
    "    mask[mask] = mask_loop\n",
    "    new_count = mask.sum()\n",
    "    print(f\"Mask size: {new_count/train_Xtime.shape[0]}\")\n",
    "    if new_count == prev_count:\n",
    "        print(\"No change in mask, stopping early\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aa8e34c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2613/2613\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 16ms/step\n",
      "Train RMSE: 0.1545\n",
      "Sqrt Variance: 0.0001\n"
     ]
    }
   ],
   "source": [
    "y_train_pred = model.predict(train_Xtime, batch_size=params['TreeTime_lstm_batch_size'])[:,0]\n",
    "rsme_train = np.sqrt(np.mean((y_train_pred - train_ytime) ** 2))\n",
    "print(f\"Train RMSE: {rsme_train:.4f}\")\n",
    "print(f\"Sqrt Variance: {np.sqrt(np.var(y_train_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fbcbaaa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean y time: 0.5085\n",
      "pred Quantile 0.5: 0.5106\n",
      "pred Quantile 0.8: 0.5124\n"
     ]
    }
   ],
   "source": [
    "print(f\"mean y time: {np.mean(train_ytime):.4f}\")\n",
    "print(f\"pred Quantile 0.5: {np.mean(train_ytime[y_train_pred > np.quantile(y_train_pred, 0.5)]):.4f}\")\n",
    "print(f\"pred Quantile 0.8: {np.mean(train_ytime[y_train_pred > np.quantile(y_train_pred, 0.8)]):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a5264131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
      "Test RMSE: 0.1250\n",
      "Sqrt Variance: 0.0001\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = model.predict(test_Xtime, batch_size=params['TreeTime_lstm_batch_size'])[:,0]\n",
    "rsme_test = np.sqrt(np.mean((y_test_pred - test_ytime) ** 2))\n",
    "print(f\"Test RMSE: {rsme_test:.4f}\")\n",
    "print(f\"Sqrt Variance: {np.sqrt(np.var(y_test_pred)):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc2a622",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = 0\n",
    "zz = 0\n",
    "zzz = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d040ef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "def objective(trial: optuna.Trial):\n",
    "    # sample hyperparameters\n",
    "    optparams = params.copy()\n",
    "    optparams.update({\n",
    "        \"TreeTime_lstm_units\": trial.suggest_categorical(\"units\", [64, 128, 256]),\n",
    "        \"TreeTime_lstm_num_layers\": trial.suggest_int(\"num_layers\", 3, 8),\n",
    "        \"TreeTime_lstm_learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 5e-2),\n",
    "        \"TreeTime_lstm_conv1d\": trial.suggest_categorical(\"use_conv1d\", [False, True]),\n",
    "        \"TreeTime_lstm_batch_size\": trial.suggest_categorical(\"batch_size\", [2**12, 2**13, 2**14]),\n",
    "    })\n",
    "    # run and report validation RMSE\n",
    "    val_rmse = run(optparams)\n",
    "    trial.report(val_rmse, step=0)\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    return val_rmse\n",
    "\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    ")\n",
    "study.optimize(objective, timeout=60*60*8)\n",
    "\n",
    "print(\"Best RMSE:\", study.best_value)\n",
    "print(\"Best params:\", study.best_trial.params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
